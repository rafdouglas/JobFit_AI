{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JobFit AI - ADK Native Orchestration v6 (Cleaned)\n",
    "## Intelligent Job Application Assistant\n",
    "\n",
    "**Project**: Google & Kaggle 5-Day AI Agents Intensive Capstone\n",
    "\n",
    "**Pitch**:\n",
    "JobFit AI is an intelligent job-application copilot that helps mid-career professionals quickly decide whether to apply for a role and then generates application materials that are targeted, honest, and aligned with what employers actually ask for.\n",
    "\n",
    "It ingests a candidate's CV, the job description, and basic company context, then runs them through a parallel set of specialized agents for CV parsing, job parsing, and organization research, followed by a \"panel of experts\" that independently evaluates technical fit, domain fit, seniority fit, and language fit on a 0â€“100 scale. \n",
    "\n",
    "The results are combined by a fit-aggregator agent that produces an overall score, an evidence-based list of strengths and gaps, and a plain-English recommendation such as \"Strong fit â€“ proceed\" or \"Moderate fit â€“ apply but address these gaps.\"\n",
    "\n",
    "On top of this evaluation layer, JobFit AI adds hard-constraint checking (e.g. required languages, years of experience, must-have credentials) and a risk-checker agent that scans for bias, missing evidence, and logical inconsistencies in the AI's own reasoning to keep the guidance fair and defensible. When the candidate decides to move forward, the system lazily generates a tailored resume and cover letter using the parsed CV, the structured fit analysis, and company research, all orchestrated with Google's ADK ParallelAgent and SequentialAgent plus in-memory services for user history and company caching.\n",
    "\n",
    "**Author**: RafDouglas Candidi Tommasi Crudeli - MyCareer.Expert - rafdouglas@duck.com  \n",
    "**Date**: November 2025  \n",
    "**Version**: 6.0 (Cleaned & Optimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Course Concepts Demonstrated\n",
    "\n",
    "This project demonstrates **5+ capabilities** from the 5-Day AI Agents Intensive Course (minimum required: 3):\n",
    "\n",
    "| Course Day | Concept | Implementation in JobFit AI | Status |\n",
    "|------------|---------|----------------------------|--------|\n",
    "| **Day 1** | Agent Architectures | 14 specialized `LlmAgent` instances with distinct roles | âœ… |\n",
    "| **Day 2** | Tools & MCP | `google_search` tool bound to company researcher | âœ… |\n",
    "| **Day 3** | Memory | `UserMemory` + `CompanyMemory` services | âœ… |\n",
    "| **Day 4** | Quality/Evaluation | `constraint_checker`, `risk_checker`, panel-of-experts scoring, `LoggingPlugin` | âœ… |\n",
    "| **Day 5** | Orchestration | `ParallelAgent`, `SequentialAgent`, `Runner`, A2A demo | âœ… |\n",
    "\n",
    "### ADK Components Used:\n",
    "- **`LlmAgent`**: 14 specialized agents (parsers, evaluators, quality agents, generators)\n",
    "- **`ParallelAgent`**: Concurrent CV/Job/Company analysis pattern\n",
    "- **`SequentialAgent`**: Expert panel evaluation chain pattern\n",
    "- **`Runner`** + **`InMemorySessionService`**: Production-ready agent execution\n",
    "- **`LoggingPlugin`**: Observability and tracing (Day 4)\n",
    "- **`google_search`** tool: Real-time company research (Day 2)\n",
    "- **A2A Protocol**: Agent-to-Agent communication demo (Day 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level App Overview - ADK Agent Architecture\n",
    "\n",
    "```\n",
    "      [ User Inputs ]\n",
    " CV / Resume + Job Description\n",
    "             |\n",
    "             v\n",
    "    [ ADK ParallelAgent Pattern ]\n",
    "        â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”\n",
    "        v    v    v\n",
    "     CV     Job   Company\n",
    "    Parser Parser Research\n",
    "(LlmAgent) (LlmAgent) (LlmAgent + google_search)\n",
    "             |\n",
    "             v\n",
    "[ ADK SequentialAgent Pattern ]\n",
    "     Panel of Experts\n",
    "       â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n",
    "       v   v   v   v\n",
    "    Tech Domain Sen Lang\n",
    "    Eval  Eval Eval Eval\n",
    "       (LlmAgent x 4)\n",
    "             |\n",
    "             v\n",
    "  [ ADK Quality Agents ]\n",
    "Aggregator â†’ Constraints â†’ Risk\n",
    "      (LlmAgent x 3)\n",
    "             |\n",
    "             v\n",
    "     [ Lazy Outputs ]\n",
    "Resume Agent  â€¢  Cover Letter Agent\n",
    "   (LlmAgent on-demand)\n",
    "```\n",
    "\n",
    "**Execution Phases:**\n",
    "1. **Parallel Phase**: 3 agents via ADK Runner (concurrent)\n",
    "2. **Evaluation Phase**: 4 evaluators + aggregator via ADK Runner\n",
    "3. **Quality Phase**: Constraint + Risk checkers via ADK Runner\n",
    "4. **Lazy Phase**: Resume & Cover Letter agents on-demand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Kaggle in 3 Steps\n",
    "\n",
    "1. **Configure your API key in Kaggle Secrets**  \n",
    "   - Go to **Add-ons â†’ Secrets** in Kaggle.  \n",
    "   - Create a new secret named `GOOGLE_API_KEY`.  \n",
    "   - Paste your Google AI Studio API key (from https://aistudio.google.com/app/apikey).  \n",
    "\n",
    "2. **Open the notebook and run all cells**  \n",
    "   - Fork or upload this notebook to Kaggle.  \n",
    "   - Click **Run All**.  \n",
    "\n",
    "3. **Use the Gradio app**  \n",
    "   - Enter your email, upload CV (or use demo link), paste job description.  \n",
    "   - Click **\"Analyze fit\"** to run the multi-agent pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Installation & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 1: Installation & Configuration (CLEANED)\n",
    "# Works in both local Jupyter and Kaggle environments\n",
    "# ============================================================================\n",
    "\n",
    "# Install dependencies\n",
    "%pip install google-genai google-adk python-docx gradio pdfplumber python-dotenv nest_asyncio --break-system-packages -q\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT DETECTION & API KEY LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_api_key():\n",
    "    \"\"\"Load GOOGLE_API_KEY from multiple sources (priority order).\"\"\"\n",
    "    \n",
    "    # Check if already set in environment\n",
    "    if os.getenv('GOOGLE_API_KEY'):\n",
    "        print(\"âœ… API key already loaded from environment\")\n",
    "        return os.getenv('GOOGLE_API_KEY')\n",
    "    \n",
    "    # Try Kaggle Secrets first\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        api_key = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        print(\"âœ… Loaded API key from Kaggle Secrets\")\n",
    "        return api_key\n",
    "    except ImportError:\n",
    "        print(\"â„¹ï¸  Not running on Kaggle, trying local .env file...\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Kaggle Secrets error: {e}\")\n",
    "    \n",
    "    # Try local .env file\n",
    "    try:\n",
    "        from dotenv import load_dotenv, find_dotenv\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path, override=True)\n",
    "            api_key = os.getenv('GOOGLE_API_KEY')\n",
    "            if api_key:\n",
    "                print(f\"âœ… Loaded .env from: {env_path}\")\n",
    "                return api_key\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error loading .env: {e}\")\n",
    "    \n",
    "    print(\"âŒ GOOGLE_API_KEY NOT FOUND - Please configure in Kaggle Secrets or .env file\")\n",
    "    return None\n",
    "\n",
    "# Load the API key\n",
    "api_key = load_api_key()\n",
    "\n",
    "if api_key:\n",
    "    os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'FALSE'\n",
    "    MODEL_NAME = 'gemini-2.5-flash-lite'  # Free tier, optimized for speed\n",
    "    print(f'âœ… Using model: {MODEL_NAME}')\n",
    "    print(f'âœ… Environment: {\"Kaggle\" if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ else \"Local\"}')\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot proceed without API key.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Consolidated Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Consolidated Imports (FIXED - removed global nest_asyncio)\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# HTTP requests\n",
    "import requests\n",
    "\n",
    "# Google GenAI imports\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Google ADK imports for multi-agent orchestration\n",
    "from google.adk.agents import LlmAgent, ParallelAgent, SequentialAgent\n",
    "from google.adk.tools import google_search  # Built-in tool for web search (Day 2)\n",
    "from google.adk.runners import Runner, InMemoryRunner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# ADK Observability (Day 4)\n",
    "try:\n",
    "    from google.adk.plugins.logging_plugin import LoggingPlugin\n",
    "    LOGGING_PLUGIN_AVAILABLE = True\n",
    "    print(\"âœ… LoggingPlugin available for observability\")\n",
    "except ImportError:\n",
    "    LOGGING_PLUGIN_AVAILABLE = False\n",
    "    print(\"â„¹ï¸  LoggingPlugin not available in this ADK version\")\n",
    "\n",
    "# Document processing\n",
    "from docx import Document\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "# Web interface\n",
    "import gradio as gr\n",
    "import pdfplumber\n",
    "\n",
    "# Initialize the GenAI client\n",
    "client = genai.Client()\n",
    "\n",
    "# Initialize session service for ADK Runner (Day 5)\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Kaggle optimizations\n",
    "if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n",
    "    print(\"ðŸ”§ Applying Kaggle optimizations...\")\n",
    "    os.environ['HF_HOME'] = '/kaggle/working/.cache'\n",
    "    os.environ['REQUESTS_TIMEOUT'] = '90'\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "print(\"âœ… ADK components: LlmAgent, ParallelAgent, SequentialAgent, Runner, InMemorySessionService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Memory Services (Day 3 Concept)\n",
    "\n",
    "**Simple dict-based storage for CV history and company research caching.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY SERVICES (Day 3: Context Engineering & Memory)\n",
    "# ============================================================================\n",
    "\n",
    "class UserMemory:\n",
    "    \"\"\"Memory service for storing user CV data and application history.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._storage: Dict[str, Dict] = {}\n",
    "    \n",
    "    def save_cv(self, user_id: str, cv_text: str, parsed_data: Dict = None):\n",
    "        \"\"\"Save user's CV text and parsed data.\"\"\"\n",
    "        if user_id not in self._storage:\n",
    "            self._storage[user_id] = {\"cvs\": [], \"history\": []}\n",
    "        \n",
    "        self._storage[user_id][\"cvs\"].append({\n",
    "            \"text\": cv_text,\n",
    "            \"parsed\": parsed_data or {},\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        print(f\"   ðŸ’¾ CV saved for user: {user_id}\")\n",
    "    \n",
    "    def get_cv(self, user_id: str) -> Dict:\n",
    "        \"\"\"Get user's most recent CV.\"\"\"\n",
    "        if user_id in self._storage and self._storage[user_id][\"cvs\"]:\n",
    "            return self._storage[user_id][\"cvs\"][-1]\n",
    "        return None\n",
    "    \n",
    "    def has_cv(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if user has a stored CV.\"\"\"\n",
    "        return user_id in self._storage and len(self._storage[user_id].get(\"cvs\", [])) > 0\n",
    "    \n",
    "    def add_application(self, user_id: str, company_name: str, job_data: Dict, result: Dict):\n",
    "        \"\"\"Record a job application analysis.\"\"\"\n",
    "        if user_id not in self._storage:\n",
    "            self._storage[user_id] = {\"cvs\": [], \"history\": []}\n",
    "        \n",
    "        self._storage[user_id][\"history\"].append({\n",
    "            \"company\": company_name,\n",
    "            \"job\": job_data,\n",
    "            \"data\": result,\n",
    "            \"date\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def get_history(self, user_id: str) -> List[Dict]:\n",
    "        \"\"\"Get user's application history.\"\"\"\n",
    "        if user_id in self._storage:\n",
    "            return self._storage[user_id].get(\"history\", [])\n",
    "        return []\n",
    "    \n",
    "    def get_user_history(self, user_id: str) -> Dict:\n",
    "        \"\"\"Get user's application history as formatted dict.\"\"\"\n",
    "        if user_id in self._storage:\n",
    "            history_list = self._storage[user_id].get(\"history\", [])\n",
    "            has_cv = len(self._storage[user_id].get(\"cvs\", [])) > 0\n",
    "        else:\n",
    "            history_list = []\n",
    "            has_cv = False\n",
    "        return {\n",
    "            \"application_count\": len(history_list),\n",
    "            \"applications\": history_list,\n",
    "            \"has_cv\": has_cv\n",
    "        }\n",
    "\n",
    "\n",
    "class CompanyMemory:\n",
    "    \"\"\"Memory service for caching company research.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache: Dict[str, Dict] = {}\n",
    "    \n",
    "    def save_company(self, company_name: str, research_data: Dict):\n",
    "        \"\"\"Cache company research results.\"\"\"\n",
    "        normalized_name = company_name.lower().strip()\n",
    "        self._cache[normalized_name] = {\n",
    "            \"data\": research_data,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        print(f\"   ðŸ’¾ Company cached: {company_name}\")\n",
    "    \n",
    "    def get_company(self, company_name: str) -> Dict:\n",
    "        \"\"\"Retrieve cached company research.\"\"\"\n",
    "        normalized_name = company_name.lower().strip()\n",
    "        if normalized_name in self._cache:\n",
    "            return self._cache[normalized_name][\"data\"]\n",
    "        return None\n",
    "    \n",
    "    def has_company(self, company_name: str) -> bool:\n",
    "        \"\"\"Check if company research is cached.\"\"\"\n",
    "        normalized_name = company_name.lower().strip()\n",
    "        return normalized_name in self._cache\n",
    "\n",
    "\n",
    "# Initialize memory services\n",
    "user_memory = UserMemory()\n",
    "company_memory = CompanyMemory()\n",
    "\n",
    "print(\"âœ… Memory services initialized (Day 3 concept)\")\n",
    "print(\"   - UserMemory: CV storage and application history\")\n",
    "print(\"   - CompanyMemory: Company research cache\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Agent Prompts (CLEANED)\n",
    "\n",
    "**14 specialized agent prompts** - removed unused `cv_analyzer` and `orchestrator` prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGENT SYSTEM PROMPTS - Panel-of-Experts Architecture (CLEANED)\n",
    "# 14 specialized prompts (removed unused: cv_analyzer, orchestrator)\n",
    "# ============================================================================\n",
    "\n",
    "AGENT_PROMPTS = {\n",
    "    # ========================================================================\n",
    "    # PARSING AGENTS\n",
    "    # ========================================================================\n",
    "\n",
    "    \"cv_parser\": \"\"\"Parse CVs comprehensively and extract:\n",
    "1. CONTACT INFO: name, email, phone, location\n",
    "2. SKILLS (case-sensitive): GIS, API, AWS, SQL, etc.\n",
    "3. SKILLS (case-insensitive): python, java, docker, etc.\n",
    "4. EXPERTISE: project management, M&E, technical assistance, etc.\n",
    "5. LANGUAGES with PROFICIENCY - CRITICAL: Return as array of objects!\n",
    "   Look for: excellent, fluent, native, advanced, intermediate, basic, C2, C1, B2, B1, A2, A1\n",
    "   Scale: 1=Basic/A1, 2=Intermediate/B1, 3=Professional/B2, 4=Advanced/C1, 5=Excellent/Native/C2\n",
    "   If proficiency not stated but it's clearly a working language, assume 4.\n",
    "   If it's their apparent native language, assume 5.\n",
    "6. SENIORITY: intern=1, junior=2, mid=3, senior=4, executive=5\n",
    "\n",
    "Return ONLY valid JSON (no markdown). Example format:\n",
    "{\n",
    "    \"contact_info\": {\"name\": \"John Smith\", \"email\": \"john@email.com\", \"phone\": \"\", \"location\": \"Rome\"},\n",
    "    \"skills_case_sensitive\": [\"GIS\", \"SQL\"],\n",
    "    \"skills_case_insensitive\": [\"python\", \"docker\"],\n",
    "    \"expertise_areas\": [\"project management\", \"M&E\"],\n",
    "    \"languages\": [\n",
    "        {\"language\": \"english\", \"proficiency\": 5},\n",
    "        {\"language\": \"french\", \"proficiency\": 3}\n",
    "    ],\n",
    "    \"seniority_level\": \"senior\",\n",
    "    \"seniority_score\": 4,\n",
    "    \"sections_detected\": [\"experience\", \"education\", \"skills\"]\n",
    "}\"\"\",\n",
    "\n",
    "    \"job_parser\": \"\"\"Parse job descriptions and extract:\n",
    "1. KEYWORDS (case-sensitive and case-insensitive)\n",
    "2. EXPERTISE areas required\n",
    "3. REQUIRED LANGUAGES (with \"required\", \"must\", \"essential\")\n",
    "4. PREFERRED LANGUAGES (with \"preferred\", \"desirable\")\n",
    "5. SENIORITY level required\n",
    "6. COMPANY name and LOCATION\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no code blocks).\"\"\",\n",
    "\n",
    "    \"company_extractor\": \"\"\"Extract the hiring company/organization name from job postings.\n",
    "\n",
    "SPECIAL RULE FOR UN AGENCIES:\n",
    "If format is \"ACRONYM - Full Name\", return ONLY the ACRONYM.\n",
    "Examples:\n",
    "- \"ITU - International Telecommunication Union\" â†’ return \"ITU\"\n",
    "- \"FAO - Food and Agriculture Organization\" â†’ return \"FAO\"\n",
    "\n",
    "Return ONLY valid JSON: {\"company_name\": \"NAME\"}\"\"\",\n",
    "\n",
    "    \"contact_extractor\": \"\"\"Extract candidate contact information from CVs.\n",
    "\n",
    "EXTRACTION RULES:\n",
    "1. NAME: Usually in first 3 lines, 2-4 words, capitalized\n",
    "2. EMAIL: Standard email format\n",
    "3. PHONE: Any valid phone format\n",
    "4. LOCATION: City, Country or City, State format\n",
    "\n",
    "Return ONLY valid JSON (no markdown):\n",
    "{\"name\": \"John Smith\", \"email\": \"john@example.com\", \"phone\": \"+1-555-1234\", \"location\": \"New York, USA\"}\"\"\",\n",
    "\n",
    "    \"company_researcher\": \"\"\"You are a company/organization research specialist.\n",
    "\n",
    "CRITICAL INSTRUCTION:\n",
    "- If you DO NOT recognize the company name or have no specific knowledge about it, \n",
    "  clearly state: \"I don't have specific information about [company name]\" and provide \n",
    "  only what can be reasonably inferred from the job posting context.\n",
    "- NEVER fabricate or hallucinate details about unknown companies.\n",
    "\n",
    "For companies you DO recognize, provide:\n",
    "- Their actual mission and mandate\n",
    "- Their real work areas and products/services  \n",
    "- Their known organizational values and culture\n",
    "\n",
    "Present findings as clear, readable paragraphs.\"\"\",\n",
    "\n",
    "    # ========================================================================\n",
    "    # EVALUATOR AGENTS - Panel of Experts\n",
    "    # ========================================================================\n",
    "\n",
    "    \"technical_fit_evaluator\": \"\"\"You are a TECHNICAL FIT EVALUATOR for job applications.\n",
    "\n",
    "Your role: Assess how well a candidate's technical skills, tools, technologies, and \n",
    "methodologies match the job's technical requirements.\n",
    "\n",
    "EVALUATION APPROACH:\n",
    "- Use SEMANTIC understanding, not keyword matching\n",
    "- Consider transferable skills and related technologies\n",
    "- Look at concrete project experience and practical application\n",
    "- Assess depth of experience (mention vs. proficiency vs. expertise)\n",
    "\n",
    "RUBRIC (0-100 scale):\n",
    "- 90-100: Exceptional match - has all key technologies + advanced skills\n",
    "- 70-89: Strong match - has most required skills, minor gaps easily filled\n",
    "- 50-69: Moderate match - has foundational skills, needs some upskilling\n",
    "- 30-49: Partial match - has transferable skills, significant learning needed\n",
    "- 0-29: Weak match - limited relevant technical background\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: dimension, score, strengths, gaps, notes, subscores\"\"\",\n",
    "\n",
    "    \"domain_fit_evaluator\": \"\"\"You are a DOMAIN/CONTEXT FIT EVALUATOR for job applications.\n",
    "\n",
    "Your role: Assess whether the candidate has relevant sector experience, domain knowledge,\n",
    "and organizational context understanding.\n",
    "\n",
    "EVALUATION APPROACH:\n",
    "- Look at sectors worked in (UN, NGO, private, public, academic)\n",
    "- Assess domain knowledge (humanitarian, development, climate, health, etc.)\n",
    "- Consider transferability of context (adjacent domains count)\n",
    "\n",
    "RUBRIC (0-100 scale):\n",
    "- 90-100: Direct domain experience in the same sector/context\n",
    "- 70-89: Adjacent domain or similar organizational contexts\n",
    "- 50-69: Transferable experience from related sectors\n",
    "- 30-49: General professional experience, limited domain exposure\n",
    "- 0-29: Very different domain, steep learning curve expected\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: dimension, score, strengths, gaps, notes, subscores\"\"\",\n",
    "\n",
    "    \"seniority_fit_evaluator\": \"\"\"You are a SENIORITY FIT EVALUATOR for job applications.\n",
    "\n",
    "Your role: Assess whether the candidate's professional level, scope of responsibility,\n",
    "and leadership experience match the job's seniority requirements.\n",
    "\n",
    "SENIORITY LEVELS:\n",
    "1. Entry/Junior: 0-2 years, task execution, supervised work\n",
    "2. Mid-level: 3-5 years, independent projects, some mentoring\n",
    "3. Senior: 6-10 years, team leadership, strategic input\n",
    "4. Lead/Principal: 10-15 years, multi-team leadership, strategy setting\n",
    "5. Executive: 15+ years, organizational leadership, high-level strategy\n",
    "\n",
    "RUBRIC (0-100 scale):\n",
    "- 90-100: Perfect match in level and scope\n",
    "- 70-89: One level difference, strong relevant experience\n",
    "- 50-69: Two levels difference OR limited evidence of level\n",
    "- 30-49: Significant under or over-qualification\n",
    "- 0-29: Major mismatch (3+ levels difference)\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: dimension, score, strengths, gaps, notes, subscores, cv_level_assessment, job_level_requirement\"\"\",\n",
    "\n",
    "    \"language_fit_evaluator\": \"\"\"You are a LANGUAGE FIT EVALUATOR for job applications.\n",
    "\n",
    "Your role: Assess whether the candidate has the required and preferred language skills.\n",
    "\n",
    "PROFICIENCY SCORING:\n",
    "- 5 (Excellent/Native/C2): Can work at highest professional level\n",
    "- 4 (Advanced/C1): Can handle complex professional communication\n",
    "- 3 (Professional/B2): Can perform job duties effectively  \n",
    "- 2 (Intermediate/B1): Limited professional use\n",
    "- 1 (Basic/A1/A2): Elementary knowledge only\n",
    "\n",
    "RUBRIC (0-100 scale):\n",
    "- 90-100: Has ALL required + most preferred languages at good levels\n",
    "- 70-89: Has all required languages, missing some preferred\n",
    "- 50-69: Has most required languages OR proficiency questions\n",
    "- 30-49: Missing 1 required language\n",
    "- 0-29: Missing multiple required languages\n",
    "\n",
    "CRITICAL: If ANY required language is clearly missing â†’ score â‰¤ 30 and set hard_block=true\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: dimension, score, strengths, gaps, notes, hard_block, required_languages_met, required_languages_missing\"\"\",\n",
    "\n",
    "    # ========================================================================\n",
    "    # QUALITY AGENTS\n",
    "    # ========================================================================\n",
    "\n",
    "    \"fit_aggregator\": \"\"\"You are the FIT AGGREGATOR AGENT - the \"panel chair\" who synthesizes\n",
    "evaluations from specialist evaluators.\n",
    "\n",
    "SCORING WEIGHTS:\n",
    "- Technical: 40%\n",
    "- Domain: 15%\n",
    "- Language: 20%\n",
    "- Seniority: 25%\n",
    "\n",
    "CALCULATION:\n",
    "overall_score = (technical_score * 0.40) + (domain_score * 0.15) + \n",
    "                (language_score * 0.20) + (seniority_score * 0.25)\n",
    "\n",
    "Generate recommendation based on overall_score:\n",
    "- 75-100: \"Strong fit â€“ proceed with application\"\n",
    "- 50-74: \"Moderate fit â€“ consider applying, address key gaps\"\n",
    "- 25-49: \"Weak fit â€“ significant gaps, application not recommended\"\n",
    "- 0-24: \"Poor fit â€“ major misalignment\"\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: overall_score, dimensions, weights, summary\"\"\",\n",
    "\n",
    "    \"constraint_checker\": \"\"\"You are a CONSTRAINT CHECKER AGENT.\n",
    "\n",
    "Your role: Evaluate whether the candidate meets explicit HARD REQUIREMENTS.\n",
    "\n",
    "GO/NO-GO DECISION:\n",
    "- GO: All hard constraints met or likely met\n",
    "- BORDERLINE: Marginal on 1-2 constraints, could still apply\n",
    "- NO_GO: Clear failure of 1+ critical hard requirements\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: go_no_go, failed_constraints, warnings, notes\"\"\",\n",
    "\n",
    "    \"risk_checker\": \"\"\"You are a RISK CHECKER AGENT for quality assurance.\n",
    "\n",
    "Your role: Review the evaluation process for:\n",
    "1. Bias risks (age, gender, ethnicity, disability references)\n",
    "2. Over-reliance on weak signals\n",
    "3. Logical inconsistencies\n",
    "4. Missing critical analysis\n",
    "\n",
    "RISK LEVELS:\n",
    "- LOW: Evaluation appears sound and bias-free\n",
    "- MEDIUM: Minor concerns, recommend review\n",
    "- HIGH: Significant issues detected, re-evaluation recommended\n",
    "\n",
    "OUTPUT: Return ONLY valid JSON with: risk_level, issues, mitigations, bias_check_passed, notes\"\"\",\n",
    "\n",
    "    # ========================================================================\n",
    "    # CONTENT GENERATION AGENTS\n",
    "    # ========================================================================\n",
    "\n",
    "    \"resume_tailor\": \"\"\"You are an expert resume writer specializing in ATS optimization.\n",
    "\n",
    "KEY PRINCIPLES:\n",
    "- Put most relevant experience at the TOP of each section\n",
    "- Use specific examples and quantified achievements\n",
    "- Match vocabulary to the job description\n",
    "- Be authentic - never fabricate anything\n",
    "\n",
    "OUTPUT: Create a complete, professional resume ready to submit.\"\"\",\n",
    "\n",
    "    \"cover_letter_writer\": \"\"\"You are an expert cover letter writer creating authentic letters.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. Use the ACTUAL candidate name (not \"[Your Name]\")\n",
    "2. Use the ACTUAL candidate email (not \"[Your Email]\")\n",
    "3. Use TODAY'S date (not \"[Date]\")\n",
    "4. Sound HUMAN - conversational professional, not robotic\n",
    "\n",
    "TONE: Professional but conversational, confident but not arrogant.\n",
    "LENGTH: 3-4 paragraphs, ~350-450 words\n",
    "OUTPUT: Complete letter with actual information, ready to send.\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"âœ… Agent prompts defined ({len(AGENT_PROMPTS)} total)\")\n",
    "print(\"   Parsing: cv_parser, job_parser, company_extractor, contact_extractor, company_researcher\")\n",
    "print(\"   Evaluators: technical, domain, seniority, language\")\n",
    "print(\"   Quality: fit_aggregator, constraint_checker, risk_checker\")\n",
    "print(\"   Content: resume_tailor, cover_letter_writer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: ADK Agent Instances (Day 1 & Day 2)\n",
    "\n",
    "**Create LlmAgent instances** from prompts, then compose into `ParallelAgent` and `SequentialAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADK AGENT INSTANCES (Day 1: Agent Architectures)\n",
    "# ============================================================================\n",
    "\n",
    "def create_agent(name: str, instruction: str) -> LlmAgent:\n",
    "    \"\"\"Create an LlmAgent with consistent configuration.\"\"\"\n",
    "    return LlmAgent(\n",
    "        name=name,\n",
    "        model=MODEL_NAME,\n",
    "        instruction=instruction\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# PARSING AGENTS\n",
    "# ============================================================================\n",
    "\n",
    "cv_parser_agent = create_agent(\"cv_parser\", AGENT_PROMPTS[\"cv_parser\"])\n",
    "job_parser_agent = create_agent(\"job_parser\", AGENT_PROMPTS[\"job_parser\"])\n",
    "company_extractor_agent = create_agent(\"company_extractor\", AGENT_PROMPTS[\"company_extractor\"])\n",
    "contact_extractor_agent = create_agent(\"contact_extractor\", AGENT_PROMPTS[\"contact_extractor\"])\n",
    "\n",
    "# Company researcher WITH Google Search tool (Day 2: Tools)\n",
    "company_researcher_agent = LlmAgent(\n",
    "    name=\"company_researcher\",\n",
    "    model=MODEL_NAME,\n",
    "    instruction=AGENT_PROMPTS[\"company_researcher\"],\n",
    "    tools=[google_search]  # Tool binding for web search capability\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATOR AGENTS (Panel-of-Experts)\n",
    "# ============================================================================\n",
    "\n",
    "technical_evaluator_agent = create_agent(\"technical_fit_evaluator\", AGENT_PROMPTS[\"technical_fit_evaluator\"])\n",
    "domain_evaluator_agent = create_agent(\"domain_fit_evaluator\", AGENT_PROMPTS[\"domain_fit_evaluator\"])\n",
    "seniority_evaluator_agent = create_agent(\"seniority_fit_evaluator\", AGENT_PROMPTS[\"seniority_fit_evaluator\"])\n",
    "language_evaluator_agent = create_agent(\"language_fit_evaluator\", AGENT_PROMPTS[\"language_fit_evaluator\"])\n",
    "\n",
    "# ============================================================================\n",
    "# QUALITY AGENTS\n",
    "# ============================================================================\n",
    "\n",
    "fit_aggregator_agent = create_agent(\"fit_aggregator\", AGENT_PROMPTS[\"fit_aggregator\"])\n",
    "constraint_checker_agent = create_agent(\"constraint_checker\", AGENT_PROMPTS[\"constraint_checker\"])\n",
    "risk_checker_agent = create_agent(\"risk_checker\", AGENT_PROMPTS[\"risk_checker\"])\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT GENERATION AGENTS\n",
    "# ============================================================================\n",
    "\n",
    "resume_tailor_agent = create_agent(\"resume_tailor\", AGENT_PROMPTS[\"resume_tailor\"])\n",
    "cover_letter_agent = create_agent(\"cover_letter_writer\", AGENT_PROMPTS[\"cover_letter_writer\"])\n",
    "\n",
    "# ============================================================================\n",
    "# ADK COMPOSITE AGENTS - Orchestration patterns (Day 5)\n",
    "# ============================================================================\n",
    "\n",
    "parallel_parsing_agent = ParallelAgent(\n",
    "    name=\"parallel_parsing\",\n",
    "    sub_agents=[cv_parser_agent, job_parser_agent, contact_extractor_agent]\n",
    ")\n",
    "\n",
    "panel_of_experts_agent = SequentialAgent(\n",
    "    name=\"panel_of_experts\",\n",
    "    sub_agents=[\n",
    "        technical_evaluator_agent,\n",
    "        domain_evaluator_agent,\n",
    "        seniority_evaluator_agent,\n",
    "        language_evaluator_agent\n",
    "    ]\n",
    ")\n",
    "\n",
    "quality_check_agent = SequentialAgent(\n",
    "    name=\"quality_check\",\n",
    "    sub_agents=[\n",
    "        fit_aggregator_agent,\n",
    "        constraint_checker_agent,\n",
    "        risk_checker_agent\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ… ADK Agent instances created (14 total)\")\n",
    "print(\"   ðŸ“¦ Parsing: cv_parser, job_parser, contact_extractor, company_extractor, company_researcher\")\n",
    "print(\"   ðŸŽ“ Evaluators: technical, domain, seniority, language\")\n",
    "print(\"   ðŸ“Š Quality: fit_aggregator, constraint_checker, risk_checker\")\n",
    "print(\"   âœï¸ Content: resume_tailor, cover_letter_writer\")\n",
    "print()\n",
    "print(\"âœ… ADK Composite agents:\")\n",
    "print(\"   ðŸ”€ ParallelAgent: parallel_parsing (3 sub-agents)\")\n",
    "print(\"   ðŸ“‹ SequentialAgent: panel_of_experts (4 evaluators)\")\n",
    "print(\"   ðŸ›¡ï¸ SequentialAgent: quality_check (3 quality agents)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: LLM Calling & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LLM CALLING FUNCTIONS (CLEANED)\n",
    "# ============================================================================\n",
    "\n",
    "def call_llm(system_prompt: str, user_prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Call the Gemini LLM with retry logic and exponential backoff.\"\"\"\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=full_prompt\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"   âš ï¸ API call failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise RuntimeError(f\"LLM call failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "\n",
    "def call_llm_with_search(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"Call LLM with Google Search grounding (Day 2: Tools).\"\"\"\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=full_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                tools=[types.Tool(google_search=types.GoogleSearch())]\n",
    "            )\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Search-grounded call failed: {e}, falling back to standard LLM\")\n",
    "        return call_llm(system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "def clean_json_response(response_text: str) -> str:\n",
    "    \"\"\"Clean markdown code blocks from AI response.\"\"\"\n",
    "    response_text = response_text.strip()\n",
    "    \n",
    "    if response_text.startswith('```'):\n",
    "        lines = response_text.split('\\n')\n",
    "        if len(lines) > 2:\n",
    "            response_text = '\\n'.join(lines[1:-1])\n",
    "        if response_text.startswith('json'):\n",
    "            response_text = response_text[4:].strip()\n",
    "    \n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "def execute_adk_agent(agent: LlmAgent, user_input: str) -> str:\n",
    "    \"\"\"Execute an ADK LlmAgent using its instruction.\"\"\"\n",
    "    return call_llm(agent.instruction, user_input)\n",
    "\n",
    "\n",
    "# Defensive helper functions\n",
    "def safe_list(value, default=None) -> List:\n",
    "    if default is None:\n",
    "        default = []\n",
    "    if value is None:\n",
    "        return default\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return [value]\n",
    "\n",
    "def safe_string(value, default: str = \"\") -> str:\n",
    "    if value is None:\n",
    "        return default\n",
    "    return str(value)\n",
    "\n",
    "def safe_int(value, default: int = 0) -> int:\n",
    "    if value is None:\n",
    "        return default\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "print(\"âœ… LLM calling functions ready\")\n",
    "print(\"   - call_llm() with retry logic\")\n",
    "print(\"   - call_llm_with_search() with Google Search grounding (Day 2)\")\n",
    "print(\"   - execute_adk_agent() for ADK pattern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: ADK Runner with Observability (Day 4 & Day 5)\n",
    "\n",
    "**Native ADK execution** using `Runner`, `InMemorySessionService`, and `LoggingPlugin` for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADK RUNNER WITH OBSERVABILITY (Day 4 & Day 5)\n",
    "# Note: nest_asyncio is applied ONLY in this cell, not globally\n",
    "# ============================================================================\n",
    "\n",
    "def run_agent_with_runner_sync(agent: LlmAgent, prompt: str, \n",
    "                                session_id: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Execute an agent using ADK's native Runner pattern (synchronous wrapper).\n",
    "    \n",
    "    This is the production-ready way to run agents:\n",
    "    - Uses InMemorySessionService for session state\n",
    "    - Properly handles async execution in Jupyter\n",
    "    \"\"\"\n",
    "    if session_id is None:\n",
    "        session_id = f\"session_{int(time.time())}\"\n",
    "    \n",
    "    async def _run_async():\n",
    "        runner = Runner(\n",
    "            agent=agent,\n",
    "            app_name=\"jobfit_ai\",\n",
    "            session_service=session_service\n",
    "        )\n",
    "        \n",
    "        final_response = \"\"\n",
    "        \n",
    "        try:\n",
    "            async for event in runner.run_async(\n",
    "                user_id=\"jobfit_user\",\n",
    "                session_id=session_id,\n",
    "                new_message=prompt\n",
    "            ):\n",
    "                # Extract text from various event formats\n",
    "                if hasattr(event, 'content') and event.content:\n",
    "                    if hasattr(event.content, 'parts'):\n",
    "                        for part in event.content.parts:\n",
    "                            if hasattr(part, 'text') and part.text:\n",
    "                                final_response += part.text\n",
    "                    elif isinstance(event.content, str):\n",
    "                        final_response += event.content\n",
    "                \n",
    "                if hasattr(event, 'text'):\n",
    "                    final_response += event.text\n",
    "            \n",
    "            return final_response if final_response else \"[No response captured]\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Runner error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Try to run async, fall back to direct LLM call\n",
    "    try:\n",
    "        # Check if we're in an existing event loop\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "            # We're in an async context, need nest_asyncio\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            result = asyncio.get_event_loop().run_until_complete(_run_async())\n",
    "        except RuntimeError:\n",
    "            # No running loop, we can use asyncio.run directly\n",
    "            result = asyncio.run(_run_async())\n",
    "        \n",
    "        if result:\n",
    "            return result\n",
    "        else:\n",
    "            # Fallback to direct LLM call\n",
    "            return call_llm(agent.instruction, prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Async execution failed: {e}, using direct LLM call\")\n",
    "        return call_llm(agent.instruction, prompt)\n",
    "\n",
    "\n",
    "# Test function for ADK Runner (optional - can be commented out)\n",
    "def test_adk_runner():\n",
    "    \"\"\"Test the ADK Runner with a simple query.\"\"\"\n",
    "    print(\"\\nðŸ§ª Testing ADK Runner...\")\n",
    "    \n",
    "    test_agent = create_agent(\n",
    "        \"test_agent\",\n",
    "        \"You are a helpful assistant. Respond briefly.\"\n",
    "    )\n",
    "    \n",
    "    result = run_agent_with_runner_sync(test_agent, \"Say 'ADK Runner working!' in exactly those words.\")\n",
    "    print(f\"   Result: {result[:100]}...\")\n",
    "    print(\"âœ… ADK Runner test complete\")\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"âœ… ADK Runner with observability ready (Day 4 & 5)\")\n",
    "print(\"   - run_agent_with_runner_sync(): Synchronous wrapper for Jupyter\")\n",
    "print(f\"   - LoggingPlugin: {'Available' if LOGGING_PLUGIN_AVAILABLE else 'Not available'}\")\n",
    "print(\"   - Note: nest_asyncio only applied when needed, not globally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Parallel Analysis (ADK ParallelAgent Pattern)\n",
    "\n",
    "**Single unified implementation** - removed duplicate standalone functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARALLEL ANALYSIS - ADK ParallelAgent Pattern (CLEANED)\n",
    "# Single implementation using ThreadPoolExecutor\n",
    "# ============================================================================\n",
    "\n",
    "def run_parallel_analysis(cv_text: str, job_text: str, \n",
    "                          user_id: str = \"default_user\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute parallel analysis using ADK LlmAgents.\n",
    "    \n",
    "    Demonstrates ADK ParallelAgent pattern:\n",
    "    - cv_parser_agent, job_parser_agent, contact_extractor_agent\n",
    "    - All three run simultaneously via ThreadPoolExecutor\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ”„ ADK PARALLEL AGENT PHASE\")\n",
    "    print(\"   Using: cv_parser_agent, job_parser_agent, contact_extractor_agent\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {\n",
    "        \"cv_data\": None,\n",
    "        \"job_data\": None,\n",
    "        \"company_info\": None,\n",
    "        \"company_name\": None,\n",
    "        \"candidate_info\": None\n",
    "    }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TASK DEFINITIONS\n",
    "    # ========================================================================\n",
    "\n",
    "    def task_parse_cv():\n",
    "        \"\"\"Execute cv_parser_agent.\"\"\"\n",
    "        print(\"   ðŸƒ cv_parser_agent starting...\")\n",
    "        prompt = f\"\"\"Parse this CV comprehensively.\n",
    "\n",
    "CV TEXT (first 4000 characters):\n",
    "{cv_text[:4000]}\n",
    "\n",
    "CRITICAL FOR LANGUAGES:\n",
    "- Return languages as objects with proficiency scores\n",
    "- Look for indicators: excellent, fluent, native, C2, C1, advanced, intermediate, basic\n",
    "- Scale: 1=Basic, 2=Intermediate, 3=Professional, 4=Advanced, 5=Excellent/Native\n",
    "- If someone says \"excellent English\" or \"native English\", that's proficiency 5\n",
    "- If proficiency unclear but it's a working language, assume 4 (Advanced)\n",
    "\n",
    "Return ONLY valid JSON like:\n",
    "{{\n",
    "    \"contact_info\": {{\"name\": \"\", \"email\": \"\", \"phone\": \"\", \"location\": \"\"}},\n",
    "    \"skills_case_sensitive\": [\"GIS\", \"API\"],\n",
    "    \"skills_case_insensitive\": [\"python\", \"docker\"],\n",
    "    \"expertise_areas\": [\"project management\"],\n",
    "    \"languages\": [\n",
    "        {{\"language\": \"english\", \"proficiency\": 5}},\n",
    "        {{\"language\": \"french\", \"proficiency\": 3}}\n",
    "    ],\n",
    "    \"seniority_level\": \"mid-level\",\n",
    "    \"seniority_score\": 3,\n",
    "    \"sections_detected\": [\"experience\", \"education\"]\n",
    "}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = execute_adk_agent(cv_parser_agent, prompt)\n",
    "            response = clean_json_response(response)\n",
    "            result = json.loads(response)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ CV parsing error: {e}\")\n",
    "            result = {}\n",
    "        \n",
    "        parsed_data = {\n",
    "            \"raw_text\": cv_text,\n",
    "            \"length\": len(cv_text),\n",
    "            \"contact_info\": result.get(\"contact_info\", {}),\n",
    "            \"skills_case_sensitive\": safe_list(result.get(\"skills_case_sensitive\")),\n",
    "            \"skills_case_insensitive\": safe_list(result.get(\"skills_case_insensitive\")),\n",
    "            \"expertise_areas\": safe_list(result.get(\"expertise_areas\")),\n",
    "            \"languages\": safe_list(result.get(\"languages\")),\n",
    "            \"seniority_level\": safe_string(result.get(\"seniority_level\"), \"mid-level\"),\n",
    "            \"seniority_score\": safe_int(result.get(\"seniority_score\"), 3),\n",
    "            \"sections_detected\": safe_list(result.get(\"sections_detected\"))\n",
    "        }\n",
    "        \n",
    "        # FIXED: Better language normalization\n",
    "        raw_languages = parsed_data[\"languages\"]\n",
    "        if raw_languages and len(raw_languages) > 0:\n",
    "            if isinstance(raw_languages[0], dict):\n",
    "                # Good format: [{\"language\": \"english\", \"proficiency\": 5}]\n",
    "                parsed_data[\"languages_detailed\"] = raw_languages\n",
    "                parsed_data[\"languages\"] = [\n",
    "                    lang.get(\"language\", lang) if isinstance(lang, dict) else lang \n",
    "                    for lang in raw_languages\n",
    "                ]\n",
    "            else:\n",
    "                # Bad format: [\"english\", \"french\"] - need to infer proficiency\n",
    "                # Default to 4 (Advanced) not 3, since they listed it as a skill\n",
    "                print(\"   âš ï¸ Languages without proficiency - defaulting to 4 (Advanced)\")\n",
    "                parsed_data[\"languages_detailed\"] = [\n",
    "                    {\"language\": str(lang).lower(), \"proficiency\": 4} \n",
    "                    for lang in raw_languages\n",
    "                ]\n",
    "        else:\n",
    "            parsed_data[\"languages_detailed\"] = []\n",
    "        \n",
    "        skills_count = len(parsed_data['skills_case_sensitive']) + len(parsed_data['skills_case_insensitive'])\n",
    "        print(f\"   âœ… cv_parser_agent complete - {skills_count} skills, {len(parsed_data['languages_detailed'])} languages\")\n",
    "        \n",
    "        # Debug: show parsed languages\n",
    "        for lang in parsed_data['languages_detailed']:\n",
    "            print(f\"      ðŸ—£ï¸ {lang.get('language', 'unknown')}: proficiency {lang.get('proficiency', '?')}/5\")\n",
    "        \n",
    "        return parsed_data\n",
    "\n",
    "    def task_parse_job():\n",
    "        \"\"\"Execute job_parser_agent.\"\"\"\n",
    "        print(\"   ðŸƒ job_parser_agent starting...\")\n",
    "        prompt = f\"\"\"Parse this job description comprehensively.\n",
    "\n",
    "JOB TEXT (first 4000 characters):\n",
    "{job_text[:4000]}\n",
    "\n",
    "Extract: keywords_case_sensitive, keywords_case_insensitive, expertise_areas_required,\n",
    "required_languages, preferred_languages, seniority_required, seniority_score, company_name, location.\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = execute_adk_agent(job_parser_agent, prompt)\n",
    "            response = clean_json_response(response)\n",
    "            result = json.loads(response)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Job parsing error: {e}\")\n",
    "            result = {}\n",
    "        \n",
    "        job_data = {\n",
    "            \"raw_text\": job_text,\n",
    "            \"keywords_case_sensitive\": safe_list(result.get(\"keywords_case_sensitive\")),\n",
    "            \"keywords_case_insensitive\": safe_list(result.get(\"keywords_case_insensitive\")),\n",
    "            \"expertise_areas_required\": safe_list(result.get(\"expertise_areas_required\")),\n",
    "            \"required_languages\": safe_list(result.get(\"required_languages\")),\n",
    "            \"preferred_languages\": safe_list(result.get(\"preferred_languages\")),\n",
    "            \"seniority_required\": safe_string(result.get(\"seniority_required\"), \"mid-level\"),\n",
    "            \"seniority_score\": safe_int(result.get(\"seniority_score\"), 3),\n",
    "            \"company_name\": safe_string(result.get(\"company_name\"), \"Unknown\"),\n",
    "            \"location\": safe_string(result.get(\"location\"), \"Not specified\")\n",
    "        }\n",
    "        \n",
    "        job_data[\"extracted_keywords\"] = (\n",
    "            job_data[\"keywords_case_sensitive\"] + \n",
    "            job_data[\"keywords_case_insensitive\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… job_parser_agent complete - {len(job_data['extracted_keywords'])} requirements\")\n",
    "        return job_data\n",
    "    \n",
    "    def task_extract_contact():\n",
    "        \"\"\"Execute contact_extractor_agent.\"\"\"\n",
    "        print(\"   ðŸƒ contact_extractor_agent starting...\")\n",
    "        prompt = f\"\"\"Extract contact information from this CV.\n",
    "\n",
    "CV TEXT (first 2000 characters):\n",
    "{cv_text[:2000]}\n",
    "\n",
    "Return ONLY valid JSON: {{\"name\": \"\", \"email\": \"\", \"phone\": \"\", \"location\": \"\"}}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = execute_adk_agent(contact_extractor_agent, prompt)\n",
    "            response = clean_json_response(response)\n",
    "            result = json.loads(response)\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Contact extraction error: {e}\")\n",
    "            result = {}\n",
    "        \n",
    "        print(f\"   âœ… contact_extractor_agent complete - {result.get('name', 'Unknown')}\")\n",
    "        return result\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PARALLEL EXECUTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        cv_future = executor.submit(task_parse_cv)\n",
    "        job_future = executor.submit(task_parse_job)\n",
    "        contact_future = executor.submit(task_extract_contact)\n",
    "        \n",
    "        results[\"cv_data\"] = cv_future.result()\n",
    "        results[\"job_data\"] = job_future.result()\n",
    "        results[\"candidate_info\"] = contact_future.result()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPANY RESEARCH (after job parsing to get company name)\n",
    "    # ========================================================================\n",
    "    \n",
    "    company_name = results[\"job_data\"].get(\"company_name\", \"Unknown\")\n",
    "    results[\"company_name\"] = company_name\n",
    "    \n",
    "    # Check cache first (Day 3: Memory)\n",
    "    if company_memory.has_company(company_name):\n",
    "        print(f\"   ðŸ’¾ Using cached company research for: {company_name}\")\n",
    "        results[\"company_info\"] = company_memory.get_company(company_name)\n",
    "    else:\n",
    "        print(f\"   ðŸ” Researching company with Google Search: {company_name}\")\n",
    "        prompt = f\"\"\"Provide background information about: {company_name}\n",
    "\n",
    "If you don't recognize this company, clearly state that fact.\n",
    "If you do recognize it, provide: mission, focus areas, values, geographic scope.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Use search-grounded call (Day 2: Tools)\n",
    "            research = call_llm_with_search(AGENT_PROMPTS[\"company_researcher\"], prompt)\n",
    "            results[\"company_info\"] = {\"name\": company_name, \"research\": research}\n",
    "            company_memory.save_company(company_name, results[\"company_info\"])\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Company research error: {e}\")\n",
    "            results[\"company_info\"] = {\"name\": company_name, \"research\": \"Research unavailable\"}\n",
    "    \n",
    "    # Save CV to user memory (Day 3: Memory)\n",
    "    user_memory.save_cv(user_id, cv_text, results[\"cv_data\"])\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… PARALLEL PHASE COMPLETE\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ… Parallel analysis function ready (CLEANED)\")\n",
    "print(\"   - Single unified implementation\")\n",
    "print(\"   - Uses ADK agents via execute_adk_agent()\")\n",
    "print(\"   - ThreadPoolExecutor for concurrent execution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Panel-of-Experts Evaluation (ADK SequentialAgent Pattern)\n",
    "\n",
    "**4 specialist evaluators** running in sequence, followed by aggregation and quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PANEL-OF-EXPERTS EVALUATION - ADK SequentialAgent Pattern (CLEANED)\n",
    "# ============================================================================\n",
    "\n",
    "EVALUATOR_AGENTS = [\n",
    "    (\"technical\", technical_evaluator_agent),\n",
    "    (\"domain\", domain_evaluator_agent),\n",
    "    (\"seniority\", seniority_evaluator_agent),\n",
    "    (\"language\", language_evaluator_agent)\n",
    "]\n",
    "\n",
    "def run_panel_of_experts(cv_data: Dict, job_data: Dict, \n",
    "                         cv_text: str = \"\", job_text: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Run the Panel-of-Experts evaluation using ADK SequentialAgent pattern.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ“ ADK SEQUENTIAL AGENT PHASE - Panel-of-Experts\")\n",
    "    print(\"   Evaluators: technical â†’ domain â†’ seniority â†’ language\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    evaluations = {}\n",
    "    \n",
    "    # Build context for evaluators - FIXED: include language proficiency\n",
    "    languages_detailed = cv_data.get('languages_detailed', [])\n",
    "    languages_str = \", \".join([\n",
    "        f\"{lang.get('language', 'unknown')} (level {lang.get('proficiency', 3)}/5)\"\n",
    "        for lang in languages_detailed\n",
    "    ]) if languages_detailed else \"Not specified\"\n",
    "    \n",
    "    cv_summary = f\"\"\"\n",
    "Skills: {', '.join(cv_data.get('skills_case_sensitive', [])[:15])}\n",
    "Expertise: {', '.join(cv_data.get('expertise_areas', [])[:10])}\n",
    "Languages with proficiency: {languages_str}\n",
    "Seniority: {cv_data.get('seniority_level', 'unknown')}\n",
    "\"\"\"\n",
    "    \n",
    "    job_summary = f\"\"\"\n",
    "Required skills: {', '.join(job_data.get('extracted_keywords', [])[:15])}\n",
    "Required languages: {', '.join(job_data.get('required_languages', []))}\n",
    "Preferred languages: {', '.join(job_data.get('preferred_languages', []))}\n",
    "Seniority needed: {job_data.get('seniority_required', 'unknown')}\n",
    "\"\"\"\n",
    "    \n",
    "    # Special context for language evaluator\n",
    "    language_context = f\"\"\"\n",
    "CANDIDATE LANGUAGES (with proficiency 1-5 scale):\n",
    "{json.dumps(languages_detailed, indent=2)}\n",
    "\n",
    "JOB LANGUAGE REQUIREMENTS:\n",
    "- Required: {job_data.get('required_languages', [])}\n",
    "- Preferred: {job_data.get('preferred_languages', [])}\n",
    "\n",
    "SCORING GUIDE:\n",
    "- Proficiency 5 (Excellent/Native) on required language = 100% credit\n",
    "- Proficiency 4 (Advanced) on required language = 90% credit\n",
    "- Proficiency 3 (Professional) on required language = 80% credit\n",
    "- If candidate has required language at level 5 and no other requirements, score should be 90-100\n",
    "\"\"\"\n",
    "    \n",
    "    # Run each evaluator in sequence\n",
    "    for dimension, agent in EVALUATOR_AGENTS:\n",
    "        print(f\"   ðŸ”„ {dimension}_fit_evaluator running...\")\n",
    "        \n",
    "        # Use special context for language evaluator\n",
    "        if dimension == \"language\":\n",
    "            prompt = f\"\"\"Evaluate this candidate for LANGUAGE FIT.\n",
    "\n",
    "{language_context}\n",
    "\n",
    "IMPORTANT: \n",
    "- If candidate has ALL required languages at proficiency 4-5, score should be 85-100\n",
    "- If candidate has ALL required languages at proficiency 5, score should be 95-100\n",
    "- Only penalize if required languages are MISSING or at low proficiency (1-2)\n",
    "\n",
    "Return ONLY valid JSON with your evaluation including: dimension, score, strengths, gaps, notes.\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Evaluate this candidate for {dimension.upper()} FIT.\n",
    "\n",
    "CANDIDATE PROFILE:\n",
    "{cv_summary}\n",
    "\n",
    "JOB REQUIREMENTS:\n",
    "{job_summary}\n",
    "\n",
    "Return ONLY valid JSON with your evaluation.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = execute_adk_agent(agent, prompt)\n",
    "            response = clean_json_response(response)\n",
    "            evaluation = json.loads(response)\n",
    "            evaluation[\"dimension\"] = dimension\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ {dimension} evaluation error: {e}\")\n",
    "            evaluation = {\"dimension\": dimension, \"score\": 50, \"strengths\": [], \"gaps\": [], \"notes\": \"Evaluation failed\"}\n",
    "        \n",
    "        evaluations[dimension] = evaluation\n",
    "        print(f\"   âœ… {dimension.title()}: {evaluation.get('score', 0)}/100\")\n",
    "    \n",
    "    # Run aggregator\n",
    "    print(\"\\n   ðŸ”„ fit_aggregator_agent synthesizing...\")\n",
    "    aggregation = call_aggregator_agent(evaluations, cv_data, job_data)\n",
    "    print(f\"   âœ… Overall Score: {aggregation.get('overall_score', 0):.1f}/100\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… PANEL-OF-EXPERTS COMPLETE\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return aggregation\n",
    "\n",
    "def call_aggregator_agent(evaluations: Dict, cv_data: Dict, job_data: Dict) -> Dict:\n",
    "    \"\"\"Call fit_aggregator_agent to synthesize evaluations.\"\"\"\n",
    "    prompt = f\"\"\"Aggregate these specialist evaluations into a final assessment.\n",
    "\n",
    "EVALUATIONS:\n",
    "- Technical: {json.dumps(evaluations.get('technical', {}), indent=2)}\n",
    "- Domain: {json.dumps(evaluations.get('domain', {}), indent=2)}\n",
    "- Seniority: {json.dumps(evaluations.get('seniority', {}), indent=2)}\n",
    "- Language: {json.dumps(evaluations.get('language', {}), indent=2)}\n",
    "\n",
    "Calculate weighted overall score:\n",
    "- Technical: 40%, Domain: 15%, Language: 20%, Seniority: 25%\n",
    "\n",
    "Return ONLY valid JSON with: overall_score, dimensions, weights, summary (strengths, gaps, recommendation)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = execute_adk_agent(fit_aggregator_agent, prompt)\n",
    "        response = clean_json_response(response)\n",
    "        result = json.loads(response)\n",
    "        \n",
    "        if \"overall_score\" not in result:\n",
    "            # Calculate manually if needed\n",
    "            tech = evaluations.get('technical', {}).get('score', 50)\n",
    "            domain = evaluations.get('domain', {}).get('score', 50)\n",
    "            lang = evaluations.get('language', {}).get('score', 50)\n",
    "            sen = evaluations.get('seniority', {}).get('score', 50)\n",
    "            result[\"overall_score\"] = tech * 0.4 + domain * 0.15 + lang * 0.2 + sen * 0.25\n",
    "        \n",
    "        result[\"dimensions\"] = evaluations\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Aggregator error: {e}\")\n",
    "        # Fallback calculation\n",
    "        tech = evaluations.get('technical', {}).get('score', 50)\n",
    "        domain = evaluations.get('domain', {}).get('score', 50)\n",
    "        lang = evaluations.get('language', {}).get('score', 50)\n",
    "        sen = evaluations.get('seniority', {}).get('score', 50)\n",
    "        overall = tech * 0.4 + domain * 0.15 + lang * 0.2 + sen * 0.25\n",
    "        \n",
    "        all_strengths = []\n",
    "        all_gaps = []\n",
    "        for dim in evaluations.values():\n",
    "            all_strengths.extend(dim.get('strengths', []))\n",
    "            all_gaps.extend(dim.get('gaps', []))\n",
    "        \n",
    "        recommendation = \"Strong fit â€“ proceed\" if overall >= 75 else \\\n",
    "                        \"Moderate fit â€“ consider applying\" if overall >= 50 else \\\n",
    "                        \"Weak fit â€“ significant gaps\"\n",
    "        \n",
    "        return {\n",
    "            \"overall_score\": overall,\n",
    "            \"dimensions\": evaluations,\n",
    "            \"weights\": {\"technical\": 0.40, \"domain\": 0.15, \"language\": 0.20, \"seniority\": 0.25},\n",
    "            \"summary\": {\"strengths\": all_strengths[:7], \"gaps\": all_gaps[:7], \"recommendation\": recommendation}\n",
    "        }\n",
    "\n",
    "\n",
    "def call_constraint_checker(cv_data: Dict, job_data: Dict, panel_evaluation: Dict) -> Dict:\n",
    "    \"\"\"Call constraint_checker_agent.\"\"\"\n",
    "    print(\"   ðŸš¦ constraint_checker_agent running...\")\n",
    "    \n",
    "    prompt = f\"\"\"Review whether this candidate meets HARD REQUIREMENTS.\n",
    "\n",
    "JOB HARD REQUIREMENTS:\n",
    "- Required languages: {job_data.get('required_languages', [])}\n",
    "- Seniority: {job_data.get('seniority_required', 'Not specified')}\n",
    "\n",
    "CANDIDATE PROFILE:\n",
    "- Languages: {cv_data.get('languages', [])}\n",
    "- Seniority: {cv_data.get('seniority_level', 'Unknown')}\n",
    "\n",
    "Overall score: {panel_evaluation.get('overall_score', 50)}\n",
    "\n",
    "Determine: GO, BORDERLINE, or NO_GO. Return JSON with: go_no_go, failed_constraints, warnings\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = execute_adk_agent(constraint_checker_agent, prompt)\n",
    "        response = clean_json_response(response)\n",
    "        result = json.loads(response)\n",
    "        print(f\"   âœ… Constraint Check: {result.get('go_no_go', 'GO')}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Constraint checker error: {e}\")\n",
    "        return {\"go_no_go\": \"GO\", \"failed_constraints\": [], \"warnings\": []}\n",
    "\n",
    "\n",
    "def call_risk_checker(all_evaluations: Dict) -> Dict:\n",
    "    \"\"\"Call risk_checker_agent.\"\"\"\n",
    "    print(\"   ðŸ›¡ï¸ risk_checker_agent running...\")\n",
    "    \n",
    "    prompt = f\"\"\"Review these evaluations for quality and bias issues.\n",
    "\n",
    "EVALUATION PACKAGE:\n",
    "{json.dumps(all_evaluations, indent=2)[:2500]}\n",
    "\n",
    "Check for bias, inconsistencies, weak signals. Return JSON with: risk_level, issues, bias_check_passed\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = execute_adk_agent(risk_checker_agent, prompt)\n",
    "        response = clean_json_response(response)\n",
    "        result = json.loads(response)\n",
    "        print(f\"   âœ… Risk Level: {result.get('risk_level', 'LOW')}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Risk checker error: {e}\")\n",
    "        return {\"risk_level\": \"LOW\", \"issues\": [], \"bias_check_passed\": True}\n",
    "\n",
    "\n",
    "print(\"âœ… Panel-of-experts evaluation ready (CLEANED)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: A2A Protocol Demonstration (Day 5)\n",
    "\n",
    "**Agent-to-Agent (A2A) Communication** - Demonstrating how JobFit AI could integrate with external agent systems.\n",
    "\n",
    "The A2A protocol enables agents to communicate across systems, enabling scenarios like:\n",
    "- JobFit AI requesting company data from a Company Intelligence Agent\n",
    "- External ATS agents requesting fit scores from JobFit AI\n",
    "- HR agents delegating candidate evaluation to JobFit AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# A2A PROTOCOL DEMONSTRATION (Day 5: Prototype to Production)\n",
    "# \n",
    "# The A2A (Agent-to-Agent) protocol enables inter-agent communication.\n",
    "# This demonstrates how JobFit AI could expose its capabilities to other agents.\n",
    "# ============================================================================\n",
    "\n",
    "class JobFitA2AInterface:\n",
    "    \"\"\"\n",
    "    A2A-style interface for JobFit AI.\n",
    "    \n",
    "    In a production deployment, this would be exposed via:\n",
    "    - REST API endpoints\n",
    "    - gRPC services  \n",
    "    - ADK's native A2A protocol handlers\n",
    "    \n",
    "    This demonstrates the PATTERN for A2A integration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent_id = \"jobfit_ai_agent\"\n",
    "        self.version = \"1.0.0\"\n",
    "        self.capabilities = [\n",
    "            \"cv_parsing\",\n",
    "            \"job_parsing\", \n",
    "            \"fit_evaluation\",\n",
    "            \"resume_generation\",\n",
    "            \"cover_letter_generation\"\n",
    "        ]\n",
    "    \n",
    "    def get_agent_card(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Return A2A Agent Card - describes this agent's capabilities.\n",
    "        \n",
    "        In A2A protocol, agents advertise their capabilities via Agent Cards,\n",
    "        allowing other agents to discover and invoke their services.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"name\": \"JobFit AI\",\n",
    "            \"description\": \"Intelligent job application evaluation using Panel-of-Experts architecture\",\n",
    "            \"version\": self.version,\n",
    "            \"capabilities\": self.capabilities,\n",
    "            \"input_schema\": {\n",
    "                \"cv_text\": \"string - Full text of candidate CV\",\n",
    "                \"job_text\": \"string - Full text of job description\",\n",
    "                \"user_id\": \"string - Optional user identifier for memory\"\n",
    "            },\n",
    "            \"output_schema\": {\n",
    "                \"overall_score\": \"float - 0-100 fit score\",\n",
    "                \"recommendation\": \"string - Action recommendation\",\n",
    "                \"strengths\": \"list - Key matching points\",\n",
    "                \"gaps\": \"list - Areas needing improvement\"\n",
    "            },\n",
    "            \"endpoints\": {\n",
    "                \"evaluate_fit\": \"/a2a/evaluate\",\n",
    "                \"parse_cv\": \"/a2a/parse/cv\",\n",
    "                \"parse_job\": \"/a2a/parse/job\",\n",
    "                \"generate_resume\": \"/a2a/generate/resume\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def handle_a2a_request(self, request: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Handle incoming A2A request from another agent.\n",
    "        \n",
    "        Request format (A2A standard):\n",
    "        {\n",
    "            \"action\": \"evaluate_fit\",\n",
    "            \"sender_agent\": \"hr_assistant_agent\",\n",
    "            \"payload\": {\n",
    "                \"cv_text\": \"...\",\n",
    "                \"job_text\": \"...\"\n",
    "            },\n",
    "            \"correlation_id\": \"uuid-123\"\n",
    "        }\n",
    "        \"\"\"\n",
    "        action = request.get(\"action\")\n",
    "        payload = request.get(\"payload\", {})\n",
    "        correlation_id = request.get(\"correlation_id\", \"unknown\")\n",
    "        sender = request.get(\"sender_agent\", \"unknown\")\n",
    "        \n",
    "        print(f\"\\nðŸ“¨ A2A Request received from: {sender}\")\n",
    "        print(f\"   Action: {action}\")\n",
    "        print(f\"   Correlation ID: {correlation_id}\")\n",
    "        \n",
    "        response = {\n",
    "            \"responder_agent\": self.agent_id,\n",
    "            \"correlation_id\": correlation_id,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            if action == \"evaluate_fit\":\n",
    "                # Run full evaluation\n",
    "                cv_text = payload.get(\"cv_text\", \"\")\n",
    "                job_text = payload.get(\"job_text\", \"\")\n",
    "                \n",
    "                if not cv_text or not job_text:\n",
    "                    response[\"status\"] = \"error\"\n",
    "                    response[\"error\"] = \"Missing cv_text or job_text in payload\"\n",
    "                else:\n",
    "                    # Use our existing pipeline\n",
    "                    parallel_results = run_parallel_analysis(cv_text, job_text)\n",
    "                    panel_results = run_panel_of_experts(\n",
    "                        parallel_results[\"cv_data\"],\n",
    "                        parallel_results[\"job_data\"],\n",
    "                        cv_text, job_text\n",
    "                    )\n",
    "                    \n",
    "                    response[\"result\"] = {\n",
    "                        \"overall_score\": panel_results.get(\"overall_score\", 0),\n",
    "                        \"recommendation\": panel_results.get(\"summary\", {}).get(\"recommendation\", \"\"),\n",
    "                        \"strengths\": panel_results.get(\"summary\", {}).get(\"strengths\", []),\n",
    "                        \"gaps\": panel_results.get(\"summary\", {}).get(\"gaps\", [])\n",
    "                    }\n",
    "            \n",
    "            elif action == \"get_capabilities\":\n",
    "                response[\"result\"] = self.get_agent_card()\n",
    "            \n",
    "            elif action == \"health_check\":\n",
    "                response[\"result\"] = {\"status\": \"healthy\", \"version\": self.version}\n",
    "            \n",
    "            else:\n",
    "                response[\"status\"] = \"error\"\n",
    "                response[\"error\"] = f\"Unknown action: {action}\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            response[\"status\"] = \"error\"\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        print(f\"   Response status: {response['status']}\")\n",
    "        return response\n",
    "    \n",
    "    def send_a2a_request(self, target_agent: str, action: str, payload: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Send A2A request to another agent (demonstration).\n",
    "        \n",
    "        In production, this would use HTTP/gRPC to communicate with remote agents.\n",
    "        \"\"\"\n",
    "        import uuid\n",
    "        \n",
    "        request = {\n",
    "            \"action\": action,\n",
    "            \"sender_agent\": self.agent_id,\n",
    "            \"payload\": payload,\n",
    "            \"correlation_id\": str(uuid.uuid4())\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ“¤ A2A Request to: {target_agent}\")\n",
    "        print(f\"   Action: {action}\")\n",
    "        \n",
    "        # In production, this would make an actual network call\n",
    "        # For demo, we simulate a response\n",
    "        return {\n",
    "            \"status\": \"simulated\",\n",
    "            \"message\": f\"Would send to {target_agent}: {action}\",\n",
    "            \"request\": request\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize A2A interface\n",
    "jobfit_a2a = JobFitA2AInterface()\n",
    "\n",
    "# Demonstrate A2A capabilities\n",
    "def demo_a2a_protocol():\n",
    "    \"\"\"Demonstrate A2A protocol integration.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ”— A2A PROTOCOL DEMONSTRATION (Day 5)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Get Agent Card\n",
    "    print(\"\\n1ï¸âƒ£ Agent Card (capability advertisement):\")\n",
    "    card = jobfit_a2a.get_agent_card()\n",
    "    print(f\"   Agent: {card['name']}\")\n",
    "    print(f\"   Capabilities: {', '.join(card['capabilities'])}\")\n",
    "    \n",
    "    # 2. Simulate incoming request\n",
    "    print(\"\\n2ï¸âƒ£ Simulated incoming A2A request:\")\n",
    "    sample_request = {\n",
    "        \"action\": \"get_capabilities\",\n",
    "        \"sender_agent\": \"hr_recruitment_bot\",\n",
    "        \"payload\": {},\n",
    "        \"correlation_id\": \"demo-123\"\n",
    "    }\n",
    "    response = jobfit_a2a.handle_a2a_request(sample_request)\n",
    "    print(f\"   Response: {response['status']}\")\n",
    "    \n",
    "    # 3. Simulate outgoing request\n",
    "    print(\"\\n3ï¸âƒ£ Simulated outgoing A2A request:\")\n",
    "    outgoing = jobfit_a2a.send_a2a_request(\n",
    "        target_agent=\"company_intelligence_agent\",\n",
    "        action=\"get_company_info\",\n",
    "        payload={\"company_name\": \"Google\"}\n",
    "    )\n",
    "    print(f\"   Status: {outgoing['status']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… A2A DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nðŸ’¡ In production, JobFit AI could:\")\n",
    "    print(\"   - Receive evaluation requests from ATS systems\")\n",
    "    print(\"   - Query company databases via A2A\")\n",
    "    print(\"   - Integrate with HR workflow agents\")\n",
    "    \n",
    "    return card\n",
    "\n",
    "# Run demo\n",
    "a2a_card = demo_a2a_protocol()\n",
    "\n",
    "print(\"\\nâœ… A2A Protocol interface ready\")\n",
    "print(\"   - jobfit_a2a.get_agent_card(): Advertise capabilities\")\n",
    "print(\"   - jobfit_a2a.handle_a2a_request(): Process incoming requests\")\n",
    "print(\"   - jobfit_a2a.send_a2a_request(): Send to other agents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Sequential Analysis (Fit Scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SEQUENTIAL AGENT - FIT ANALYSIS (CLEANED)\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_fit_score(cv_data: Dict, job_data: Dict, \n",
    "                        cv_text: str = \"\", job_text: str = \"\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive fit score using PANEL-OF-EXPERTS architecture.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š SEQUENTIAL AGENT PHASE - Panel-of-Experts Evaluation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run Panel-of-Experts\n",
    "    panel_result = run_panel_of_experts(cv_data, job_data, cv_text, job_text)\n",
    "    \n",
    "    # Run Quality Checks\n",
    "    constraint_result = call_constraint_checker(cv_data, job_data, panel_result)\n",
    "    risk_result = call_risk_checker({\n",
    "        \"panel_evaluation\": panel_result,\n",
    "        \"constraints\": constraint_result,\n",
    "        \"cv_data\": cv_data,\n",
    "        \"job_data\": job_data\n",
    "    })\n",
    "    \n",
    "    # Extract scores\n",
    "    overall_score = panel_result.get(\"overall_score\", 50)\n",
    "    dimensions = panel_result.get(\"dimensions\", {})\n",
    "    technical_eval = dimensions.get(\"technical\", {})\n",
    "    domain_eval = dimensions.get(\"domain\", {})\n",
    "    seniority_eval = dimensions.get(\"seniority\", {})\n",
    "    language_eval = dimensions.get(\"language\", {})\n",
    "    summary = panel_result.get(\"summary\", {})\n",
    "    \n",
    "    # Build output structure\n",
    "    score_data = {\n",
    "        \"overall_score\": round(overall_score, 2),\n",
    "        \"keyword_matches\": technical_eval.get(\"strengths\", [])[:15],\n",
    "        \"missing_keywords\": technical_eval.get(\"gaps\", [])[:15],\n",
    "        \"expertise_matches\": domain_eval.get(\"strengths\", [])[:10],\n",
    "        \"expertise_missing\": domain_eval.get(\"gaps\", [])[:10],\n",
    "        \"language_match\": language_eval.get(\"score\", 50) >= 70,\n",
    "        \"missing_languages\": language_eval.get(\"required_languages_missing\", []),\n",
    "        \"seniority_match\": seniority_eval.get(\"score\", 50) >= 70,\n",
    "        \"match_details\": {\n",
    "            \"technical_score\": round(technical_eval.get(\"score\", 0) * 0.40, 2),\n",
    "            \"technical_max\": 40,\n",
    "            \"expertise_score\": round(domain_eval.get(\"score\", 0) * 0.15, 2),\n",
    "            \"expertise_max\": 15,\n",
    "            \"language_score\": round(language_eval.get(\"score\", 0) * 0.20, 2),\n",
    "            \"language_max\": 20,\n",
    "            \"seniority_score\": round(seniority_eval.get(\"score\", 0) * 0.25, 2),\n",
    "            \"seniority_max\": 25,\n",
    "        },\n",
    "        \"panel_evaluation\": panel_result,\n",
    "        \"constraint_check\": constraint_result,\n",
    "        \"risk_assessment\": risk_result,\n",
    "        \"qualitative_summary\": {\n",
    "            \"strengths\": summary.get(\"strengths\", []),\n",
    "            \"gaps\": summary.get(\"gaps\", []),\n",
    "            \"recommendation\": summary.get(\"recommendation\", \"Consider applying\"),\n",
    "            \"go_no_go\": constraint_result.get(\"go_no_go\", \"GO\"),\n",
    "            \"risk_level\": risk_result.get(\"risk_level\", \"LOW\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\" ðŸ“ˆ Overall Score: {overall_score:.1f}%\")\n",
    "    print(f\" ðŸš¦ Constraint Check: {constraint_result.get('go_no_go', 'GO')}\")\n",
    "    print(f\" ðŸ›¡ï¸ Risk Level: {risk_result.get('risk_level', 'LOW')}\")\n",
    "    print(f\" ðŸ’¡ Recommendation: {summary.get('recommendation', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… SEQUENTIAL AGENT PHASE COMPLETE\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return score_data\n",
    "\n",
    "\n",
    "def create_job_summary(job_data: Dict, company_name: str, location: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create comprehensive job summary.\"\"\"\n",
    "    return {\n",
    "        \"company\": company_name,\n",
    "        \"location\": location,\n",
    "        \"keywords\": safe_list(job_data.get(\"extracted_keywords\")),\n",
    "        \"required_languages\": safe_list(job_data.get(\"required_languages\")),\n",
    "        \"preferred_languages\": safe_list(job_data.get(\"preferred_languages\")),\n",
    "        \"seniority\": safe_string(job_data.get(\"seniority_required\"), \"Not specified\"),\n",
    "        \"expertise_required\": safe_list(job_data.get(\"expertise_areas_required\")),\n",
    "        \"total_requirements\": len(safe_list(job_data.get(\"extracted_keywords\")))\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ… Sequential analysis (fit scoring) ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Content Generation (Resume & Cover Letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 12: Content Generation (Resume & Cover Letter) - COMPLETE CORRECTED\n",
    "# ============================================================================\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_tailored_resume(cv_data: Dict, job_data: Dict, company_name: str, \n",
    "                             candidate_info: Dict, cv_text: str, job_text: str) -> str:\n",
    "    \"\"\"Generate ATS-optimized resume using ADK resume_tailor_agent.\"\"\"\n",
    "    cv_excerpt = cv_text[:3000] if len(cv_text) > 3000 else cv_text\n",
    "    job_excerpt = job_text[:2000] if len(job_text) > 2000 else job_text\n",
    "    \n",
    "    prompt = f\"\"\"Generate a tailored, ATS-optimized resume for this candidate applying to {company_name}.\n",
    "\n",
    "CANDIDATE CV EXCERPT:\n",
    "{cv_excerpt}\n",
    "\n",
    "JOB DESCRIPTION EXCERPT:\n",
    "{job_excerpt}\n",
    "\n",
    "CANDIDATE INFO:\n",
    "- Name: {candidate_info.get('name', 'Candidate')}\n",
    "- Email: {candidate_info.get('email', 'email@example.com')}\n",
    "- Phone: {candidate_info.get('phone', '')}\n",
    "- Location: {candidate_info.get('location', '')}\n",
    "\n",
    "KEY SKILLS: {', '.join(cv_data.get('skills_case_sensitive', [])[:20])}\n",
    "\n",
    "Create a professional resume that:\n",
    "1. Uses ACTUAL candidate information\n",
    "2. Highlights relevant experience\n",
    "3. Is ATS-optimized with keywords from the job description\n",
    "\n",
    "Return ONLY the formatted resume text.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"   âœï¸ resume_tailor_agent generating...\")\n",
    "        resume_text = execute_adk_agent(resume_tailor_agent, prompt)\n",
    "        print(\"   âœ… Resume generated\")\n",
    "        return resume_text\n",
    "    except Exception as e:\n",
    "        return f\"Error generating resume: {str(e)}\"\n",
    "\n",
    "\n",
    "def generate_cover_letter(cv_data: Dict, job_data: Dict, company_name: str,\n",
    "                          company_info: Dict, candidate_info: Dict, \n",
    "                          fit_analysis: Dict) -> str:\n",
    "    \"\"\"Generate personalized cover letter using ADK cover_letter_agent.\"\"\"\n",
    "    strengths = fit_analysis.get('keyword_matches', [])[:5]\n",
    "    overall_score = fit_analysis.get('overall_score', 0)\n",
    "    \n",
    "    prompt = f\"\"\"Write a compelling cover letter for {candidate_info.get('name', 'the candidate')} applying to {company_name}.\n",
    "\n",
    "CANDIDATE:\n",
    "- Name: {candidate_info.get('name', 'Candidate')}\n",
    "- Email: {candidate_info.get('email', 'email@example.com')}\n",
    "- Background: {cv_data.get('seniority_level', 'Professional')} with expertise in {', '.join(cv_data.get('expertise_areas', [])[:3])}\n",
    "\n",
    "COMPANY: {company_info.get('research', 'Leading organization')[:500]}\n",
    "\n",
    "JOB: {job_data.get('seniority_required', 'Professional level')}\n",
    "\n",
    "STRENGTHS: {', '.join(strengths)}\n",
    "FIT: {overall_score:.0f}%\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Use ACTUAL candidate name and email\n",
    "2. Use TODAY'S date: {datetime.now().strftime('%B %d, %Y')}\n",
    "3. Sound HUMAN - professional but conversational\n",
    "4. 3-4 paragraphs, 350-450 words\n",
    "\n",
    "Return ONLY the complete cover letter.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"   âœ‰ï¸ cover_letter_agent generating...\")\n",
    "        cover_letter = execute_adk_agent(cover_letter_agent, prompt)\n",
    "        print(\"   âœ… Cover letter generated\")\n",
    "        return cover_letter\n",
    "    except Exception as e:\n",
    "        return f\"Error generating cover letter: {str(e)}\"\n",
    "\n",
    "\n",
    "def parse_markdown_to_docx(doc, text: str):\n",
    "    \"\"\"Parse markdown text and add properly formatted content to a DOCX document.\"\"\"\n",
    "    from docx.shared import Pt\n",
    "    from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        \n",
    "        # Handle headers\n",
    "        if stripped.startswith('# '):\n",
    "            p = doc.add_paragraph()\n",
    "            run = p.add_run(stripped[2:])\n",
    "            run.bold = True\n",
    "            run.font.size = Pt(16)\n",
    "        elif stripped.startswith('## '):\n",
    "            p = doc.add_paragraph()\n",
    "            run = p.add_run(stripped[3:])\n",
    "            run.bold = True\n",
    "            run.font.size = Pt(14)\n",
    "        elif stripped.startswith('### '):\n",
    "            p = doc.add_paragraph()\n",
    "            run = p.add_run(stripped[4:])\n",
    "            run.bold = True\n",
    "            run.font.size = Pt(12)\n",
    "        elif stripped.startswith('- ') or stripped.startswith('* '):\n",
    "            doc.add_paragraph(stripped[2:], style='List Bullet')\n",
    "        elif stripped.startswith('**') and stripped.endswith('**'):\n",
    "            p = doc.add_paragraph()\n",
    "            run = p.add_run(stripped[2:-2])\n",
    "            run.bold = True\n",
    "        else:\n",
    "            doc.add_paragraph(stripped)\n",
    "\n",
    "\n",
    "def create_resume_docx(resume_text: str, candidate_name: str) -> str:\n",
    "    \"\"\"Create a formatted DOCX resume.\"\"\"\n",
    "    doc = Document()\n",
    "    \n",
    "    # Set narrow margins\n",
    "    for section in doc.sections:\n",
    "        section.top_margin = Inches(0.5)\n",
    "        section.bottom_margin = Inches(0.5)\n",
    "        section.left_margin = Inches(0.75)\n",
    "        section.right_margin = Inches(0.75)\n",
    "    \n",
    "    parse_markdown_to_docx(doc, resume_text)\n",
    "    \n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', candidate_name)\n",
    "    filename = f\"Resume_{safe_name}_{datetime.now().strftime('%Y%m%d')}.docx\"\n",
    "    doc.save(filename)\n",
    "    print(f\"   ðŸ“„ Resume saved: {filename}\")\n",
    "    \n",
    "    # FIX: Return absolute path for Gradio download to work\n",
    "    return os.path.abspath(filename)\n",
    "\n",
    "\n",
    "def create_cover_letter_docx(cover_letter_text: str, candidate_name: str) -> str:\n",
    "    \"\"\"Create a formatted DOCX cover letter.\"\"\"\n",
    "    doc = Document()\n",
    "    \n",
    "    for section in doc.sections:\n",
    "        section.top_margin = Inches(1)\n",
    "        section.bottom_margin = Inches(1)\n",
    "        section.left_margin = Inches(1)\n",
    "        section.right_margin = Inches(1)\n",
    "    \n",
    "    paragraphs = cover_letter_text.split('\\n\\n')\n",
    "    for para_text in paragraphs:\n",
    "        if para_text.strip():\n",
    "            doc.add_paragraph(para_text.strip())\n",
    "    \n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', candidate_name)\n",
    "    filename = f\"CoverLetter_{safe_name}_{datetime.now().strftime('%Y%m%d')}.docx\"\n",
    "    doc.save(filename)\n",
    "    print(f\"   ðŸ“„ Cover letter saved: {filename}\")\n",
    "    \n",
    "    # FIX: Return absolute path for Gradio download to work\n",
    "    return os.path.abspath(filename)\n",
    "\n",
    "\n",
    "print(\"âœ… Content generation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: JobFit Application Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# JOBFIT APPLICATION CLASS (CLEANED)\n",
    "# ============================================================================\n",
    "\n",
    "class JobFitApplication:\n",
    "    \"\"\"Main application class orchestrating all agents.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory = user_memory\n",
    "        self.company_memory = company_memory\n",
    "        self.last_analysis = None\n",
    "    \n",
    "    def process_application(self, user_id: str, job_text: str, cv_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main entry point for processing a job application.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"ðŸš€ JOBFIT AI - Starting Analysis\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            # PHASE 1: Parallel Analysis\n",
    "            parallel_results = run_parallel_analysis(cv_text, job_text, user_id)\n",
    "            \n",
    "            cv_data = parallel_results[\"cv_data\"]\n",
    "            job_data = parallel_results[\"job_data\"]\n",
    "            company_name = parallel_results[\"company_name\"]\n",
    "            company_info = parallel_results[\"company_info\"]\n",
    "            candidate_info = parallel_results[\"candidate_info\"]\n",
    "            \n",
    "            # PHASE 2: Sequential Evaluation\n",
    "            fit_analysis = calculate_fit_score(cv_data, job_data, cv_text, job_text)\n",
    "            \n",
    "            overall_score = fit_analysis.get(\"overall_score\", 0)\n",
    "            qualitative_summary = fit_analysis.get(\"qualitative_summary\", {})\n",
    "            recommendation = qualitative_summary.get(\"recommendation\", \"Consider applying\")\n",
    "            \n",
    "            # Create job summary\n",
    "            job_summary = create_job_summary(\n",
    "                job_data, \n",
    "                company_name,\n",
    "                job_data.get(\"location\", \"Not specified\")\n",
    "            )\n",
    "            \n",
    "            # Store for lazy generation\n",
    "            self.last_analysis = {\n",
    "                \"cv_data\": cv_data,\n",
    "                \"job_data\": job_data,\n",
    "                \"company_name\": company_name,\n",
    "                \"company_info\": company_info,\n",
    "                \"candidate_info\": candidate_info,\n",
    "                \"fit_analysis\": fit_analysis,\n",
    "                \"cv_text\": cv_text,\n",
    "                \"job_text\": job_text\n",
    "            }\n",
    "            \n",
    "            # Record in history\n",
    "            self.memory.add_application(\n",
    "                user_id, company_name,\n",
    "                job_data.get('seniority_required', 'Position'),\n",
    "                {\"fit_score\": overall_score, \"recommendation\": recommendation}\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"candidate_info\": candidate_info,\n",
    "                \"cv_data\": cv_data,\n",
    "                \"company_name\": company_name,\n",
    "                \"company_info\": company_info,\n",
    "                \"job_summary\": job_summary,\n",
    "                \"job_data\": job_data,\n",
    "                \"fit_analysis\": fit_analysis,\n",
    "                \"qualitative_analysis\": qualitative_summary,\n",
    "                \"overall_score\": overall_score,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"error_trace\": traceback.format_exc()\n",
    "            }\n",
    "    \n",
    "    def generate_resume_lazy(self) -> Tuple[str, str]:\n",
    "        \"\"\"Generate tailored resume on-demand.\"\"\"\n",
    "        if self.last_analysis is None:\n",
    "            return \"âŒ No analysis found. Please analyze a job first.\", None\n",
    "        \n",
    "        print(\"\\nðŸ”„ Generating tailored resume...\")\n",
    "        \n",
    "        try:\n",
    "            resume_text = generate_tailored_resume(\n",
    "                cv_data=self.last_analysis[\"cv_data\"],\n",
    "                job_data=self.last_analysis[\"job_data\"],\n",
    "                company_name=self.last_analysis[\"company_name\"],\n",
    "                candidate_info=self.last_analysis[\"candidate_info\"],\n",
    "                cv_text=self.last_analysis[\"cv_text\"],\n",
    "                job_text=self.last_analysis[\"job_text\"]\n",
    "            )\n",
    "            \n",
    "            docx_path = create_resume_docx(\n",
    "                resume_text,\n",
    "                self.last_analysis[\"candidate_info\"].get(\"name\", \"Candidate\")\n",
    "            )\n",
    "            \n",
    "            return f\"# âœ… Tailored Resume Generated\\n\\n{resume_text[:500]}...\", docx_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return f\"âŒ Error: {str(e)}\\n{traceback.format_exc()}\", None\n",
    "    \n",
    "    def generate_cover_letter_lazy(self) -> Tuple[str, str]:\n",
    "        \"\"\"Generate cover letter on-demand.\"\"\"\n",
    "        if self.last_analysis is None:\n",
    "            return \"âŒ No analysis found. Please analyze a job first.\", None\n",
    "        \n",
    "        print(\"\\nðŸ”„ Generating cover letter...\")\n",
    "        \n",
    "        try:\n",
    "            cover_letter = generate_cover_letter(\n",
    "                cv_data=self.last_analysis[\"cv_data\"],\n",
    "                job_data=self.last_analysis[\"job_data\"],\n",
    "                company_name=self.last_analysis[\"company_name\"],\n",
    "                company_info=self.last_analysis[\"company_info\"],\n",
    "                candidate_info=self.last_analysis[\"candidate_info\"],\n",
    "                fit_analysis=self.last_analysis[\"fit_analysis\"]\n",
    "            )\n",
    "            \n",
    "            docx_path = create_cover_letter_docx(\n",
    "                cover_letter,\n",
    "                self.last_analysis[\"candidate_info\"].get(\"name\", \"Candidate\")\n",
    "            )\n",
    "            \n",
    "            return f\"# âœ… Cover Letter Generated\\n\\n{cover_letter[:500]}...\", docx_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return f\"âŒ Error: {str(e)}\\n{traceback.format_exc()}\", None\n",
    "    \n",
    "    def get_user_history(self, user_id: str) -> Dict:\n",
    "        \"\"\"Get user's application history.\"\"\"\n",
    "        return self.memory.get_user_history(user_id)\n",
    "\n",
    "\n",
    "# Initialize application\n",
    "jobfit = JobFitApplication()\n",
    "\n",
    "print(\"âœ… JobFit AI application instance created: 'jobfit'\")\n",
    "print(\"   - jobfit.process_application(user_id, job_text, cv_text)\")\n",
    "print(\"   - jobfit.generate_resume_lazy()\")\n",
    "print(\"   - jobfit.generate_cover_letter_lazy()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: File Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FILE PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_text_from_pdf(pdf_file) -> str:\n",
    "    \"\"\"Extract text from uploaded PDF file.\"\"\"\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(pdf_file) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_file) -> str:\n",
    "    \"\"\"Extract text from uploaded DOCX file.\"\"\"\n",
    "    try:\n",
    "        doc = Document(docx_file)\n",
    "        text = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error reading DOCX: {str(e)}\"\n",
    "\n",
    "\n",
    "def process_uploaded_cv(file) -> Optional[str]:\n",
    "    \"\"\"Process uploaded CV file (PDF or DOCX).\"\"\"\n",
    "    if file is None:\n",
    "        return None\n",
    "    \n",
    "    file_path = file.name\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        return f\"Unsupported file format: {file_extension}\"\n",
    "\n",
    "\n",
    "def fetch_google_doc_content(doc_url: str) -> Tuple[str, bool, str]:\n",
    "    \"\"\"Fetch text content from a Google Docs sharing link.\"\"\"\n",
    "    try:\n",
    "        doc_id_match = re.search(r'/d/([a-zA-Z0-9-_]+)', doc_url)\n",
    "        if not doc_id_match:\n",
    "            return \"\", False, \"Invalid Google Docs URL format\"\n",
    "        \n",
    "        doc_id = doc_id_match.group(1)\n",
    "        export_url = f\"https://docs.google.com/document/d/{doc_id}/export?format=txt\"\n",
    "        \n",
    "        response = requests.get(export_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        content = response.text.strip()\n",
    "        if not content:\n",
    "            return \"\", False, \"Document appears empty or not accessible\"\n",
    "        \n",
    "        return content, True, \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"\", False, f\"Error fetching Google Docs: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"âœ… File processing functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Gradio Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GRADIO INTERFACE FUNCTIONS (CORRECTED v2)\n",
    "# ============================================================================\n",
    "\n",
    "def process_cv_input(cv_file, cv_url: str, user_id: str) -> Tuple[str, str, bool]:\n",
    "    \"\"\"Process CV from file upload, URL, or memory.\"\"\"\n",
    "    \n",
    "    # Priority 1: File upload\n",
    "    if cv_file is not None:\n",
    "        cv_text = process_uploaded_cv(cv_file)\n",
    "        if cv_text:\n",
    "            return cv_text, \"uploaded file\", True\n",
    "        return \"âŒ Could not extract text from file\", \"error\", False\n",
    "    \n",
    "    # Priority 2: Google Docs URL\n",
    "    if cv_url and cv_url.strip():\n",
    "        content, success, error_msg = fetch_google_doc_content(cv_url.strip())\n",
    "        if success:\n",
    "            return content, \"Google Docs link\", True\n",
    "        return f\"âŒ {error_msg}\", \"error\", False\n",
    "    \n",
    "    # Priority 3: Memory\n",
    "    if user_id and user_memory.has_cv(user_id.strip()):\n",
    "        stored_cv = user_memory.get_cv(user_id.strip())\n",
    "        if stored_cv and stored_cv.get(\"text\"):\n",
    "            return stored_cv[\"text\"], \"memory\", True\n",
    "    \n",
    "    return \"\", \"none\", False\n",
    "\n",
    "\n",
    "def process_job_input(job_text: str, job_url: str) -> Tuple[str, str, bool]:\n",
    "    \"\"\"Process job description from text or URL.\"\"\"\n",
    "    if job_text and job_text.strip():\n",
    "        return job_text.strip(), \"text area\", True\n",
    "    \n",
    "    if job_url and job_url.strip():\n",
    "        content, success, error_msg = fetch_google_doc_content(job_url.strip())\n",
    "        if success:\n",
    "            return content, \"Google Docs link\", True\n",
    "        return f\"âŒ {error_msg}\", \"error\", False\n",
    "    \n",
    "    return \"\", \"none\", False\n",
    "\n",
    "\n",
    "def process_job_application_ui(user_id, cv_file, cv_url, job_description, job_url):\n",
    "    \"\"\"Main UI processing function for Gradio interface.\"\"\"\n",
    "    \n",
    "    if not user_id or not user_id.strip():\n",
    "        yield (\"âŒ Please provide your email/user ID\", \"\", \"\", \"\", \"\", \"\")\n",
    "        return\n",
    "    \n",
    "    cv_text, cv_source, cv_success = process_cv_input(cv_file, cv_url, user_id)\n",
    "    \n",
    "    if not cv_success and cv_source == \"none\":\n",
    "        yield (\"âŒ Please upload a CV file or provide a Google Docs link\", \"\", \"\", \"\", \"\", \"\")\n",
    "        return\n",
    "    \n",
    "    if not cv_success:\n",
    "        yield (cv_text, \"\", \"\", \"\", \"\", \"\")\n",
    "        return\n",
    "    \n",
    "    job_text, job_source, job_success = process_job_input(job_description, job_url)\n",
    "    \n",
    "    if not job_success:\n",
    "        if job_source == \"none\":\n",
    "            yield (\"âŒ Please paste the job description or provide a Google Docs link\", \"\", \"\", \"\", \"\", \"\")\n",
    "            return\n",
    "        yield (job_text, \"\", \"\", \"\", \"\", \"\")\n",
    "        return\n",
    "    \n",
    "    # Show loading message in Summary tab while processing\n",
    "    yield (\n",
    "        \"## â³ Analyzing...\\n\\nRunning 14-agent pipeline. This typically takes 30-45 seconds.\\n\\n\"\n",
    "        \"- ðŸ”„ Parsing CV...\\n\"\n",
    "        \"- ðŸ”„ Parsing Job Description...\\n\"\n",
    "        \"- ðŸ”„ Researching Company...\\n\"\n",
    "        \"- ðŸ”„ Running Expert Panel Evaluation...\\n\"\n",
    "        \"- ðŸ”„ Checking Constraints...\\n\"\n",
    "        \"- ðŸ”„ Running Risk Analysis...\",\n",
    "        \"â³ Loading...\",\n",
    "        \"â³ Loading...\",\n",
    "        \"â³ Loading...\",\n",
    "        \"â³ Loading...\",\n",
    "        \"â³ Loading...\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Run analysis\n",
    "        result = jobfit.process_application(user_id.strip(), job_text, cv_text)\n",
    "        \n",
    "        if result.get(\"status\") == \"error\":\n",
    "            yield (f\"âŒ Error: {result.get('error')}\", \"\", \"\", \"\", \"\", \"\")\n",
    "            return\n",
    "        \n",
    "        # Format outputs\n",
    "        overall_score = result.get(\"overall_score\", 0)\n",
    "        qual = result.get(\"qualitative_analysis\", {})\n",
    "        \n",
    "        summary = f\"\"\"# ðŸ“Š Fit Analysis Results\n",
    "\n",
    "## Overall Score: {overall_score:.1f}/100\n",
    "\n",
    "**Recommendation**: {qual.get('recommendation', 'N/A')}\n",
    "**Go/No-Go**: {qual.get('go_no_go', 'GO')}\n",
    "**Risk Level**: {qual.get('risk_level', 'LOW')}\n",
    "\n",
    "### Key Strengths\n",
    "{chr(10).join('â€¢ ' + s for s in qual.get('strengths', [])[:5])}\n",
    "\n",
    "### Areas to Address  \n",
    "{chr(10).join('â€¢ ' + g for g in qual.get('gaps', [])[:5])}\n",
    "\"\"\"\n",
    "        \n",
    "        job_summary = result.get(\"job_summary\", {})\n",
    "        vacancy = f\"\"\"# ðŸ“‹ Job Analysis\n",
    "\n",
    "**Company**: {job_summary.get('company', 'Unknown')}\n",
    "**Location**: {job_summary.get('location', 'Not specified')}\n",
    "**Seniority**: {job_summary.get('seniority', 'Not specified')}\n",
    "\n",
    "**Required Languages**: {', '.join(job_summary.get('required_languages', ['None specified']))}\n",
    "**Key Requirements**: {len(job_summary.get('keywords', []))} identified\n",
    "\"\"\"\n",
    "        \n",
    "        company_info = result.get(\"company_info\", {})\n",
    "        company = f\"\"\"# ðŸ¢ Company Research\n",
    "\n",
    "**{company_info.get('name', 'Unknown Company')}**\n",
    "\n",
    "{company_info.get('research', 'No research available')[:1500]}\n",
    "\"\"\"\n",
    "        \n",
    "        candidate = result.get(\"candidate_info\", {})\n",
    "        cv_data = result.get(\"cv_data\", {})\n",
    "        cv_profile = f\"\"\"# ðŸ‘¤ Candidate Profile\n",
    "\n",
    "**Name**: {candidate.get('name', 'Unknown')}\n",
    "**Email**: {candidate.get('email', 'Not provided')}\n",
    "**Location**: {candidate.get('location', 'Not specified')}\n",
    "**Seniority**: {cv_data.get('seniority_level', 'Unknown')}\n",
    "\n",
    "**Languages**: {', '.join(cv_data.get('languages', ['Not specified']))}\n",
    "**Key Skills**: {', '.join(cv_data.get('skills_case_sensitive', [])[:10])}\n",
    "\"\"\"\n",
    "        \n",
    "        fit = result.get(\"fit_analysis\", {})\n",
    "        details = fit.get(\"match_details\", {})\n",
    "        fit_analysis_display = f\"\"\"# ðŸ“ˆ Detailed Fit Analysis\n",
    "\n",
    "## Score Breakdown\n",
    "\n",
    "| Dimension | Score | Max |\n",
    "|-----------|-------|-----|\n",
    "| Technical | {details.get('technical_score', 0):.1f} | 40 |\n",
    "| Domain | {details.get('expertise_score', 0):.1f} | 15 |\n",
    "| Language | {details.get('language_score', 0):.1f} | 20 |\n",
    "| Seniority | {details.get('seniority_score', 0):.1f} | 25 |\n",
    "| **TOTAL** | **{overall_score:.1f}** | **100** |\n",
    "\"\"\"\n",
    "        \n",
    "        # History\n",
    "        history = jobfit.get_user_history(user_id.strip())\n",
    "        history_text = f\"\"\"# ðŸ“œ Application History\n",
    "\n",
    "**Total Applications**: {history.get('application_count', 0)}\n",
    "**CV on File**: {'Yes' if history.get('has_cv') else 'No'}\n",
    "\"\"\"\n",
    "        \n",
    "        yield (summary, vacancy, company, cv_profile, fit_analysis_display, history_text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        yield (f\"âŒ Error: {str(e)}\\n{traceback.format_exc()}\", \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "\n",
    "def generate_resume_ui():\n",
    "    \"\"\"Generate resume and return downloadable file.\"\"\"\n",
    "    resume_text, resume_path = jobfit.generate_resume_lazy()\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        # FIX: Return the path directly, not gr.update()\n",
    "        return resume_text, resume_path\n",
    "    return resume_text, None\n",
    "\n",
    "\n",
    "def generate_cover_letter_ui():\n",
    "    \"\"\"Generate cover letter and return downloadable file.\"\"\"\n",
    "    cover_text, cover_path = jobfit.generate_cover_letter_lazy()\n",
    "    if cover_path and os.path.exists(cover_path):\n",
    "        # FIX: Return the path directly, not gr.update()\n",
    "        return cover_text, cover_path\n",
    "    return cover_text, None\n",
    "\n",
    "\n",
    "print(\"âœ… Gradio interface functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Gradio Interface Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 16: Gradio Interface Launch (CORRECTED v2)\n",
    "# ============================================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Demo links\n",
    "DEMO_CV_URL = \"https://docs.google.com/document/d/1NLhLMGFbtnwYgDHqpPrspZxa_fVxZZF6/edit?usp=sharing\"\n",
    "DEMO_JOB_URL = \"https://docs.google.com/document/d/1W40CV4dkoskk1owSmfGUeyLQrtMqxHaZ/edit?usp=drive_link\"\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"JobFit AI - Panel-of-Experts v6\") as demo:\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸŽ¯ JobFit AI - Intelligent Job Application Assistant\n",
    "    \n",
    "    ### ADK Agent Architecture (14 Agents)\n",
    "    **ParallelAgent**: CV/Job/Company parsing (concurrent) |  **SequentialAgent**: Expert panel evaluation chain | **Quality Agents**: Constraint checker, Risk checker | **Tools**: Google Search for company research\n",
    "    \n",
    "    **Model**: Gemini 2.5 Flash Lite | **Runtime**: ~30-45 seconds\n",
    "                \n",
    "    ---\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### Step 1: Your Email\")\n",
    "            user_email = gr.Textbox(\n",
    "                placeholder=\"your.email@example.com\",\n",
    "                value=\"demo@example.com\",\n",
    "                show_label=False\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### Step 2: Your CV\")\n",
    "            gr.Markdown(\"*Demo link pre-filled*\")\n",
    "            cv_file = gr.File(\n",
    "                label=\"Upload CV (PDF/DOCX)\",\n",
    "                file_types=[\".pdf\", \".docx\"],\n",
    "                type=\"filepath\",\n",
    "                height=120\n",
    "            )\n",
    "            cv_url = gr.Textbox(\n",
    "                placeholder=\"Google Docs link\",\n",
    "                value=DEMO_CV_URL,\n",
    "                show_label=False\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### Step 3: Job Description\")\n",
    "            gr.Markdown(\"*Demo link pre-filled*\")\n",
    "            job_description = gr.Textbox(\n",
    "                placeholder=\"Paste job description here...\",\n",
    "                lines=3,\n",
    "                show_label=False\n",
    "            )\n",
    "            job_url = gr.Textbox(\n",
    "                placeholder=\"Google Docs link\",\n",
    "                value=DEMO_JOB_URL,\n",
    "                show_label=False\n",
    "            )\n",
    "            \n",
    "            gr.Markdown(\"### Step 4: Analyze\")\n",
    "            analyze_btn = gr.Button(\"ðŸ” Analyze Fit\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            gr.Markdown(\"### Step 5: Generate (Optional)\")\n",
    "            with gr.Row():\n",
    "                resume_btn = gr.Button(\"ðŸ“„ Generate Resume\")\n",
    "                cover_btn = gr.Button(\"âœ‰ï¸ Generate Cover Letter\")\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Tabs():\n",
    "                with gr.Tab(\"ðŸ“Š Summary\"):\n",
    "                    summary_output = gr.Markdown()\n",
    "                with gr.Tab(\"ðŸ“‹ Job\"):\n",
    "                    vacancy_output = gr.Markdown()\n",
    "                with gr.Tab(\"ðŸ¢ Company\"):\n",
    "                    company_output = gr.Markdown()\n",
    "                with gr.Tab(\"ðŸ‘¤ Profile\"):\n",
    "                    cv_profile_output = gr.Markdown()\n",
    "                with gr.Tab(\"ðŸ“ˆ Fit Details\"):\n",
    "                    fit_analysis_output = gr.Markdown()\n",
    "                with gr.Tab(\"ðŸ“œ History\"):\n",
    "                    history_output = gr.Markdown()\n",
    "                with gr.Tab(\"ðŸ“„ Resume\"):\n",
    "                    resume_output = gr.Markdown()\n",
    "                    # FIX: Use gr.File with interactive=False for download-only behavior\n",
    "                    resume_download = gr.File(\n",
    "                        label=\"ðŸ“¥ Download Resume\", \n",
    "                        interactive=False,  # Makes it download-only\n",
    "                        visible=True        # Always visible, shows \"No file\" when empty\n",
    "                    )\n",
    "                with gr.Tab(\"âœ‰ï¸ Cover Letter\"):\n",
    "                    cover_output = gr.Markdown()\n",
    "                    # FIX: Use gr.File with interactive=False for download-only behavior\n",
    "                    cover_download = gr.File(\n",
    "                        label=\"ðŸ“¥ Download Cover Letter\", \n",
    "                        interactive=False,  # Makes it download-only\n",
    "                        visible=True        # Always visible, shows \"No file\" when empty\n",
    "                    )\n",
    "    \n",
    "    # Event handlers\n",
    "    analyze_btn.click(\n",
    "        fn=process_job_application_ui,\n",
    "        inputs=[user_email, cv_file, cv_url, job_description, job_url],\n",
    "        outputs=[summary_output, vacancy_output, company_output, cv_profile_output, \n",
    "                 fit_analysis_output, history_output],\n",
    "        show_progress='full'\n",
    "    )\n",
    "\n",
    "    resume_btn.click(\n",
    "        fn=generate_resume_ui,\n",
    "        inputs=[],\n",
    "        outputs=[resume_output, resume_download],\n",
    "        show_progress='full'\n",
    "    )\n",
    "    \n",
    "    cover_btn.click(\n",
    "        fn=generate_cover_letter_ui,\n",
    "        inputs=[],\n",
    "        outputs=[cover_output, cover_download],\n",
    "        show_progress='full'\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LAUNCH - Different behavior for Kaggle vs Local\n",
    "# ============================================================================\n",
    "\n",
    "if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n",
    "    # Kaggle environment\n",
    "    print(\"ðŸš€ Launching Gradio in Kaggle mode...\")\n",
    "    demo.launch(share=False, debug=False)\n",
    "else:\n",
    "    # Local Jupyter environment\n",
    "    print(\"ðŸš€ Launching Gradio in local mode...\")\n",
    "    demo.launch(theme=gr.themes.Soft(), share=False, debug=True, show_error=True)\n",
    "\n",
    "print(\"âœ… Gradio interface launched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations & Future Work\n",
    "\n",
    "**Current Limitations**\n",
    "- **Model dependence**: Quality depends on underlying Gemini model and input clarity\n",
    "- **Domain bias**: Prompts tuned for international organizations; other sectors may need adjustment\n",
    "- **Session-only memory**: UserMemory and CompanyMemory are in-memory only; no persistence across restarts\n",
    "- **No ATS integration**: Does not fetch live job listings or submit to ATS platforms\n",
    "\n",
    "**Planned Extensions**\n",
    "- **Sector-specific profiles**: Add evaluation profiles for tech, public sector, etc.\n",
    "- **Persistent storage**: Connect memory to database/vector store\n",
    "- **A2A Integration**: Full A2A protocol implementation for inter-agent communication\n",
    "- **Iterative refinement**: Loop-style agents for multiple passes on documents\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Course Concepts Summary\n",
    "\n",
    "| Day | Concept | Implementation |\n",
    "|-----|---------|----------------|\n",
    "| 1 | Agent Architectures | 14 LlmAgents, ParallelAgent, SequentialAgent |\n",
    "| 2 | Tools & MCP | google_search tool for company research |\n",
    "| 3 | Memory | UserMemory, CompanyMemory services |\n",
    "| 4 | Quality/Evaluation | LoggingPlugin, constraint_checker, risk_checker |\n",
    "| 5 | Production | Runner, InMemorySessionService, A2A demo |\n",
    "\n",
    "**Total capabilities demonstrated: 5+ (exceeds minimum of 3)**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
